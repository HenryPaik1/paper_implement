{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calss\n",
    "# 1. common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "import sentencepiece as spm\n",
    "\n",
    "VOCAB_PATH = \"/home/henry/Documents/wrapper/source\"\n",
    "vocab_file = f\"{VOCAB_PATH}/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'n_output': 2}\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "config = dict({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12,\n",
    "    \"n_output\": 2,\n",
    "    \"device\": device\n",
    "})\n",
    "print(config)\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    \"\"\"\n",
    "    key_vector의 pad열은 모두 0으로 padding\n",
    "    - row: query token\n",
    "    - col: key token\n",
    "    \n",
    "    params\n",
    "    - seq_q, seq_k: [bs, len_seq]\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # key vector에 masking한 것을, len_q만큼 늘려줌(row wise)\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attn pad mask example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 2, 1],\n",
       "        [2, 2, 2, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_key_seq = torch.randint(0,3, size=(2, 4)) # [bs, q_seq_len]\n",
    "sample_key_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 2, 2, 1]],\n",
       "\n",
       "        [[2, 2, 2, 0]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_key_seq.unsqueeze(1) # [2,4] -> [2, 1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 2, 2, 1],\n",
       "         [0, 2, 2, 1],\n",
       "         [0, 2, 2, 1],\n",
       "         [0, 2, 2, 1]],\n",
       "\n",
       "        [[2, 2, 2, 0],\n",
       "         [2, 2, 2, 0],\n",
       "         [2, 2, 2, 0],\n",
       "         [2, 2, 2, 0]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_key_seq.unsqueeze(1).expand(2, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True, False, False, False],\n",
       "         [ True, False, False, False],\n",
       "         [ True, False, False, False]],\n",
       "\n",
       "        [[False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_pad = sample_key_seq.eq(0).unsqueeze(1).expand(2, 4, 4) # attn_pad\n",
    "attn_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_attn_decoder_mask example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mask = torch.ones_like(sample_key_seq).unsqueeze(-2).expand(sample_key_seq.size(0), sample_key_seq.size(1), sample_key_seq.size(1))\n",
    "decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1],\n",
       "         [0, 0, 1, 1],\n",
       "         [0, 0, 0, 1],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mask.triu(1) # convert upper diagonal part into 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder part masking example\n",
    "-  logical operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True, False, False, False],\n",
       "         [ True, False, False, False],\n",
       "         [ True, False, False, False]],\n",
       "\n",
       "        [[False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mask.triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True],\n",
       "         [ True, False,  True,  True],\n",
       "         [ True, False, False,  True],\n",
       "         [ True, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'or' opeartion\n",
    "# True+False==True; True+True==True; False+False==False; <- or(mask==True)==True\n",
    "attn_pad + decoder_mask.triu(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True],\n",
       "         [ True, False,  True,  True],\n",
       "         [ True, False, False,  True],\n",
       "         [ True, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mask = torch.gt(attn_pad + decoder_mask.triu(1), 0) # gt, not eq: mask the part of value, true \n",
    "decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    variables\n",
    "    - inputs: output of attn layer(attn_outputs)\n",
    "        - shape:[bs, len_query_seq, d_hidn]\n",
    "    return\n",
    "    - output\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = nn.gelu()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoswiseFeedForwardNet example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 9., 3., 7., 9., 3., 2., 4.],\n",
       "         [4., 4., 1., 2., 4., 5., 1., 9.],\n",
       "         [5., 9., 4., 8., 9., 5., 4., 5.],\n",
       "         [5., 7., 2., 3., 5., 2., 8., 2.]],\n",
       "\n",
       "        [[1., 1., 9., 6., 1., 2., 9., 9.],\n",
       "         [4., 3., 8., 7., 5., 3., 1., 1.],\n",
       "         [5., 6., 7., 3., 5., 8., 7., 1.],\n",
       "         [8., 8., 8., 3., 3., 9., 2., 4.]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multihead output: [bs, seq_len, d_hidn]\n",
    "# how to apply 1d conv to NLP in torch: https://gist.github.com/spro/c87cc706625b8a54e604fb1024106556\n",
    "attn_outputs = torch.randint(1, 10, size=[2, 4, 8]).float()\n",
    "attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 4])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input dim [bs, d_hidn, seq_len]\n",
    "conv1d_1st = nn.Conv1d(in_channels=8, out_channels=128, kernel_size=1)(attn_outputs.transpose(2, 1))\n",
    "conv1d_1st.shape # [bs, out_dim, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 4])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_2nd = nn.Conv1d(in_channels=128, out_channels=8, kernel_size=1)(conv1d_1st)\n",
    "conv1d_2nd.shape # [bs, d_hidn, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_2nd.transpose(2,1).shape # [bs, seq_len, d_hidn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    variables\n",
    "    - V, Q: [bs, len_seq, d_hidn]\n",
    "    - attn_prob: [bs, n_head, len_query_seq, len_key_seq]\n",
    "    - context: [bs, n_head, len_query_seq, d_hidn]\n",
    "    calculate\n",
    "    - scores: Q * K.T = [bs, len_seq, d_hidn] * [bs, d_hidn, len_seq]\n",
    "        - shape: [bs, len_seq, len_seq]\n",
    "    - attn_prob: dropout(softmax(score))\n",
    "    - context: torch.matmul(attn_prob, Value vector)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9) # softmax에서 sum!=0 위해 -1e9\n",
    "        \n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        \n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaled dot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False,  True]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_pad # [bs, q_seq_len, k_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 8., 6., 5.],\n",
       "         [2., 9., 7., 2.],\n",
       "         [5., 9., 9., 8.],\n",
       "         [9., 9., 4., 4.]],\n",
       "\n",
       "        [[9., 9., 8., 7.],\n",
       "         [4., 8., 6., 8.],\n",
       "         [8., 3., 2., 7.],\n",
       "         [6., 8., 7., 5.]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randint_like(attn_pad, 2, 10, dtype=torch.float)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.0000e+00,  8.0000e+00,  6.0000e+00, -1.0000e+09],\n",
       "         [ 2.0000e+00,  9.0000e+00,  7.0000e+00, -1.0000e+09],\n",
       "         [ 5.0000e+00,  9.0000e+00,  9.0000e+00, -1.0000e+09],\n",
       "         [ 9.0000e+00,  9.0000e+00,  4.0000e+00, -1.0000e+09]],\n",
       "\n",
       "        [[ 9.0000e+00,  9.0000e+00,  8.0000e+00, -1.0000e+09],\n",
       "         [ 4.0000e+00,  8.0000e+00,  6.0000e+00, -1.0000e+09],\n",
       "         [ 8.0000e+00,  3.0000e+00,  2.0000e+00, -1.0000e+09],\n",
       "         [ 6.0000e+00,  8.0000e+00,  7.0000e+00, -1.0000e+09]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill_(attn_pad, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5876e-02, 8.6681e-01, 1.1731e-01, 0.0000e+00],\n",
       "         [8.0254e-04, 8.8009e-01, 1.1911e-01, 0.0000e+00],\n",
       "         [9.0747e-03, 4.9546e-01, 4.9546e-01, 0.0000e+00],\n",
       "         [4.9832e-01, 4.9832e-01, 3.3577e-03, 0.0000e+00]],\n",
       "\n",
       "        [[4.2232e-01, 4.2232e-01, 1.5536e-01, 0.0000e+00],\n",
       "         [1.5876e-02, 8.6681e-01, 1.1731e-01, 0.0000e+00],\n",
       "         [9.9087e-01, 6.6764e-03, 2.4561e-03, 0.0000e+00],\n",
       "         [9.0031e-02, 6.6524e-01, 2.4473e-01, 0.0000e+00]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_prob = nn.Softmax(dim=-1)(scores.float())\n",
    "attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 5., 9., 6.],\n",
       "         [2., 9., 5., 5.],\n",
       "         [5., 4., 6., 5.],\n",
       "         [8., 2., 7., 4.]],\n",
       "\n",
       "        [[4., 9., 6., 8.],\n",
       "         [7., 6., 4., 6.],\n",
       "         [6., 4., 3., 3.],\n",
       "         [6., 5., 6., 7.]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.randint(2, 10, size=(2,4,4)).float()\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 1.7336, 0.2346, 0.0000],\n",
       "         [0.0000, 1.7602, 0.0000, 0.0000],\n",
       "         [0.0181, 0.9909, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0067, 0.0000]],\n",
       "\n",
       "        [[0.8446, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.9817, 0.0134, 0.0000, 0.0000],\n",
       "         [0.0000, 1.3305, 0.4895, 0.0000]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Dropout()(attn_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 6.9365, 12.8820, 10.9002,  9.9093],\n",
       "         [ 1.9933,  4.9832,  8.9698,  5.9799]],\n",
       "\n",
       "        [[ 5.2429,  8.8446,  6.0000,  7.6893],\n",
       "         [ 1.5347,  1.2243,  0.8944,  0.9579],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 9.3134,  7.9829,  5.3219,  7.9829]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(nn.Dropout()(attn_prob), V)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    weight mat\n",
    "    - W_Q, W_K, W_K: [d_hidn, n_head*d_head]\n",
    "    \n",
    "    vectors\n",
    "    - q_s, k_s, v_s: [bs, n_head, seq_len, d_head]\n",
    "    \n",
    "    calculate\n",
    "    - eg. W_Q(Q): Q * W_Q = [bs, seq_len, d_hidn] * [d_hidn, n_head*d_head] \n",
    "                            = [bs, seq_len, n_head*d_head] \n",
    "                            <- ([seq_len * d_head] mat = embedded Q가 n_head만큼 있음) * bs\n",
    "    - eg. W_Q(Q).view(bs, -1, n_head, d_head).transpose(1, 2) = (bs, n_head, -1=seq_len, d_head) \n",
    "    \n",
    "    return\n",
    "    - output: [bs, len_query_seq, d_hidn]\n",
    "    - attn_prob: [bs, n_head, len_query_seq, len_key_seq]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # n_head의 K, Q, V 한번에 생성\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    # n_head의 K, Q, V 한번에 생성\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head) # [bs, seq_len, n_head * d_head]\n",
    "        output = self.linear(context) # 각 head의 output은 position wise sum이므로 해당 calculate 가능\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multihead forward part example\n",
    "- eg.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [bs, len_seq, embed_dim=d_hidn]\n",
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 1024])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = input_vector # [bs, seq_len, d_hidn]\n",
    "Qs = nn.Linear(in_features=128, out_features=8*128)(Q)\n",
    "Qs.shape # [bs, seq_len, n_head*d_hidn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eg.2: context vector 변환\n",
    "    - reshape & transpose했을 때 `[19, 23,  7, 92, 66, 10, 15, 73, 19,  5, 81, 80, 49, 92, 79]` 변화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[19, 23,  7, 92, 66, 10, 15, 73, 19,  5, 81, 80, 49, 92, 79],\n",
       "        [29, 78, 13, 54, 29, 49, 56, 55,  5, 47, 87, 92, 34, 18, 75],\n",
       "        [72, 68, 10, 57, 34, 47, 46, 63,  3, 49, 40, 87, 79, 92, 41],\n",
       "        [ 5, 96, 76, 14, 73, 64, 90, 41, 69, 18, 95, 75, 96, 72, 30]],\n",
       "\n",
       "       [[91, 42, 91, 98,  1, 17, 64, 33, 35, 26,  2, 94, 33, 23, 53],\n",
       "        [43, 61, 49, 26, 86, 10, 61, 90, 89, 24, 68, 21, 28, 12, 64],\n",
       "        [57, 68, 77, 74, 10,  3, 99, 29, 81, 97, 24, 65, 63, 91, 73],\n",
       "        [93,  1, 47, 60, 90, 49, 21, 90, 96, 41, 29, 53, 59, 57, 42]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.randint(1, 100, size=(2,4,3*5)) # [bs, seq_len, n_head * d_head]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[19, 23,  7, 92, 66],\n",
       "         [10, 15, 73, 19,  5],\n",
       "         [81, 80, 49, 92, 79]],\n",
       "\n",
       "        [[29, 78, 13, 54, 29],\n",
       "         [49, 56, 55,  5, 47],\n",
       "         [87, 92, 34, 18, 75]],\n",
       "\n",
       "        [[72, 68, 10, 57, 34],\n",
       "         [47, 46, 63,  3, 49],\n",
       "         [40, 87, 79, 92, 41]],\n",
       "\n",
       "        [[ 5, 96, 76, 14, 73],\n",
       "         [64, 90, 41, 69, 18],\n",
       "         [95, 75, 96, 72, 30]]],\n",
       "\n",
       "\n",
       "       [[[91, 42, 91, 98,  1],\n",
       "         [17, 64, 33, 35, 26],\n",
       "         [ 2, 94, 33, 23, 53]],\n",
       "\n",
       "        [[43, 61, 49, 26, 86],\n",
       "         [10, 61, 90, 89, 24],\n",
       "         [68, 21, 28, 12, 64]],\n",
       "\n",
       "        [[57, 68, 77, 74, 10],\n",
       "         [ 3, 99, 29, 81, 97],\n",
       "         [24, 65, 63, 91, 73]],\n",
       "\n",
       "        [[93,  1, 47, 60, 90],\n",
       "         [49, 21, 90, 96, 41],\n",
       "         [29, 53, 59, 57, 42]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(2,-1,3,5) # [bs, seq_len, n_head, d_head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[19, 23,  7, 92, 66],\n",
       "         [29, 78, 13, 54, 29],\n",
       "         [72, 68, 10, 57, 34],\n",
       "         [ 5, 96, 76, 14, 73]],\n",
       "\n",
       "        [[10, 15, 73, 19,  5],\n",
       "         [49, 56, 55,  5, 47],\n",
       "         [47, 46, 63,  3, 49],\n",
       "         [64, 90, 41, 69, 18]],\n",
       "\n",
       "        [[81, 80, 49, 92, 79],\n",
       "         [87, 92, 34, 18, 75],\n",
       "         [40, 87, 79, 92, 41],\n",
       "         [95, 75, 96, 72, 30]]],\n",
       "\n",
       "\n",
       "       [[[91, 42, 91, 98,  1],\n",
       "         [43, 61, 49, 26, 86],\n",
       "         [57, 68, 77, 74, 10],\n",
       "         [93,  1, 47, 60, 90]],\n",
       "\n",
       "        [[17, 64, 33, 35, 26],\n",
       "         [10, 61, 90, 89, 24],\n",
       "         [ 3, 99, 29, 81, 97],\n",
       "         [49, 21, 90, 96, 41]],\n",
       "\n",
       "        [[ 2, 94, 33, 23, 53],\n",
       "         [68, 21, 28, 12, 64],\n",
       "         [24, 65, 63, 91, 73],\n",
       "         [29, 53, 59, 57, 42]]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(A.reshape(2,-1,3,5), axes=[0,2,1,3]) # [bs, n_head, seq_len, d_head]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. encoder clasas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    variables\n",
    "    - attn_outputs: [bs, lne_query_seq, d_hidn]\n",
    "    - ffn_outputs: [bs, lne_query_seq, d_hidn]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        \n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        \n",
    "        return ffn_outputs, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    variable\n",
    "    - inputs: [bs, seq_len] <- inputs before embedding elems of which refer to voc_idx\n",
    "    - outputs: embeded output; [bs, seq_len, d_hidn];\n",
    "    - final outputs: result context vector of attn layers; [bs, seq_len, d_hidn];\n",
    "    - attn_mask: [bs, seq_q_len, seq_k_len]\n",
    "    - attn_prob: [bs, seq_q_len, d_hidn] <- 최종 결과물\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        position = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        position.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(position)\n",
    "        \n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            outputs, attn_prob = layer(outputs, attn_mask) # outputs = ffn_outputs\n",
    "            attn_probs.append(attn_prob)\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### position example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8, 4, 6, 4, 9, 3, 2],\n",
       "        [5, 1, 2, 1, 2, 1, 8, 2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randint(1,10,size=(2, 8))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(inputs.size(1)).expand(2, 8).contiguous()+1\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 8, 4, 6, 4, 9, 0, 0],\n",
       "        [5, 1, 2, 1, 2, 1, 8, 2]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to exmple padded inputs\n",
    "inputs[0,-2:] = 0\n",
    "inputs = inputs.contiguous()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the padding 0 select correspoding value \n",
    "# from postion embedding look up table\n",
    "pos_mask = inputs.eq(0)\n",
    "position.masked_fill(pos_mask, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    1st attn layer: self attention layer\n",
    "    - Q, K, V = decoder inputs\n",
    "    2nd attn layer: decoder encoder attention layer\n",
    "    - Q, K, V = 1st attn layer outputs, encoder outputs, encoder outputs\n",
    "    \n",
    "    MultiHeadAttention return\n",
    "    - attn_outputs: [bs, seq_q_len, d_hidn]\n",
    "    - attn_prob: [bs, n_head, seq_q_len, seq_k_len]\n",
    "    - dec_enc_attn_prob: [bs, n_head, n_dec_seq, n_enc_seq]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "        self.pos_feedforward = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
    "        self_attn_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask) \n",
    "        self_attn_outputs = self.layer_norm1(dec_inputs + self_attn_outputs) # LaterNorm(residual)\n",
    "        \n",
    "        dec_enc_attn_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_attn_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_attn_outputs = self.layer_norm2(self_attn_outputs + dec_enc_attn_outputs) # LaterNorm(residual)\n",
    "        \n",
    "        ffn_outputs = self.pos_feedforward(dec_enc_attn_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_attn_outputs + ffn_outputs)\n",
    "        \n",
    "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    variables\n",
    "    - dec_inputs: [bs, seq_len]\n",
    "    - dec_outputs: token & postion embedded inputs\n",
    "        - shape: [bs, seq_len, d_hidn]\n",
    "    - dec_attn_pad_mask: mask padded key seq\n",
    "    - dec_attn_decoder_mask: mask triangular part\n",
    "    - dec_self_attn_mask: dec_attn_pad_mask + dec_attn_decoder_mask\n",
    "        - or(+) operation both \"dec_attn_pad_mask\" and \"dec_attn_decoder_mask\"\n",
    "    - dec_enc_attn_mask: masking key vector(=enc_inputs)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        position = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype)\\\n",
    "                        .expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        position.masked_fill_(pos_mask, 0)\n",
    "        \n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(position)\n",
    "        \n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad) # get_attn_pad_mask(seq_q, seq_k, i_pad)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs) # 대각선 기준 위 mask\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        \n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "        \n",
    "        self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, d_dec_seq, n_enc_seq)\n",
    "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "        return dec_outputs, self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    variables\n",
    "    - enc_outputs: [bs, len_enc_seq, d_hidn]\n",
    "    - enc_self_attn_probs: [bs, n_head, len_enc_seq, len_enc_seq]\n",
    "    - dec_outputs: [bs, len_seq, d_hidn]\n",
    "    - dec_self_attn_probs: [bs, n_head, len_dec_seq, len_dec_seq]\n",
    "    - dec_enc_dec_porbs: [bs, n_head, len_dec_seq, len_enc_seq]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        \n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "        \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Dataset \n",
    "- Naver movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset class tutorial\n",
    "    - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    note: decoding inputs have \"[BOS]\" only, because it predict 1, 0 sentimental label\n",
    "    self.vocab: sentencepiece object\n",
    "    self.labels: sentiment lable\n",
    "        - eg. [1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "    self.sentence: review sentence\n",
    "        - eg. [ 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아',]\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, data_fn):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentence = []\n",
    "        \n",
    "        with open(fn, 'r') as f:\n",
    "            for i, json_data in enumerate(f):\n",
    "                if i == 50:\n",
    "                    break\n",
    "                json_data = json.loads(json_data)\n",
    "                self.labels.append(json_data['label'])\n",
    "                self.sentence.append([vocab.piece_to_id(tok) for tok in json_data['doc']])\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentence)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.labels[idx]), \n",
    "               torch.tensor(self.sentence[idx]),\n",
    "               torch.tensor([self.vocab.piece_to_id(\"[BOS]\")])) # input only [BOS] into Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch_data):\n",
    "    \"\"\"\n",
    "    batch_data: n data(n=batch size) from Dataset \n",
    "    return\n",
    "    - label\n",
    "    - enc_inputs(token_ids): [bs, seq_len]\n",
    "    - dec_inputs(BOS_token): [bs, seq_len]\n",
    "    \"\"\"\n",
    "    label, token_ids, BOS_token = list(zip(*batch_data))\n",
    "    \n",
    "    token_ids = nn.utils.rnn.pad_sequence(token_ids, batch_first=True, padding_value=0)\n",
    "    BOS_token = nn.utils.rnn.pad_sequence(BOS_token, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return [torch.stack(label, dim=0), token_ids, BOS_token] # [label, encoder inputs, decoder inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "train_dataset = CustomDataset(vocab, data_fn='ratings_train.json')\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, collate_fn=custom_collate_fn)\n",
    "test_dataset = CustomDataset(vocab, data_fn='ratings_test.json')\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate dataset example code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []; sentence = []; d = []; line_cnt = 0; data = [];\n",
    "fn = 'ratings_test.txt'\n",
    "with open(fn) as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line_cnt += 1\n",
    "        if line_cnt == 1:\n",
    "            continue\n",
    "        if not line:\n",
    "            break\n",
    "        id_, sent, label = line.split('\\t')\n",
    "        label = label.split('\\n')[0]\n",
    "        labels.append(int(label))\n",
    "        sentence.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       " ['굳 ㅋ',\n",
       "  'GDNTOPCLASSINTHECLUB',\n",
       "  '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아',\n",
       "  '지루하지는 않은데 완전 막장임... 돈주고 보기에는....',\n",
       "  '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??',\n",
       "  '음악이 주가 된, 최고의 음악영화',\n",
       "  '진정한 쓰레기',\n",
       "  '마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다',\n",
       "  '갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다',\n",
       "  '이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10], sentence[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert preprocess data to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = {'id': id_, 'doc': vocab.encode_as_pieces(sent), 'label':label}\n",
    "with open('ratings_train.json', 'w') as f:\n",
    "    f.write(json.dumps(target_data))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json data: {'id': 9976970, 'doc': ['▁아', '▁더', '빙', '..', '▁진', '짜', '▁', '짜', '증', '나', '네', '요', '▁목', '소', '리'], 'label': 0} \n",
      "tok_to_id: [26, 228, 4365, 1920, 132, 4351, 3587, 4351, 3922, 3628, 3857, 3760, 266, 3678, 3614]\n"
     ]
    }
   ],
   "source": [
    "fn = '../ratings_train.json'\n",
    "with open(fn, 'r') as f:\n",
    "    iterator = iter(f)\n",
    "    sample_data = next(iterator) # iter json data\n",
    "\n",
    "data = json.loads(sample_data)\n",
    "print('json data:', data, '\\ntok_to_id:', [vocab.piece_to_id(tok) for tok in data['doc']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. clf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSentClf(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer classification\n",
    "    - average or max pooling over hidden_dim -> linear(in_feature=-1, out_feature=n_class)\n",
    "    \n",
    "    torch.max() return values and indices\n",
    "    - dec_outputs(values): [bs, d_hidn] <- max pooling\n",
    "    \n",
    "    logits: [bs, n_output]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = Transformer(self.config)\n",
    "        self.feedforward = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "        \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\n",
    "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\n",
    "        logits = self.feedforward(dec_outputs)\n",
    "        \n",
    "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "    \n",
    "    n_word_total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            labels, enc_inputs, dec_inputs = map(lambda x: x.to(config.device), batch)\n",
    "            \n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            logits = outputs[0] # logist = feedforard(dec_outputs): [bs, n_out]\n",
    "            val, idx = logits.max(axis=1) # eg. binary clf: [3, 5] -> idx: [1]; idx shape: [bs]\n",
    "            \n",
    "            match = torch.eq(idx, labels).detach().cpu()\n",
    "            matchs.extend(match.detach().cpu())\n",
    "            \n",
    "            running_accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "            print(running_accuracy)\n",
    "            \n",
    "    return running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, n_epoch, model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda x: x.to(config.device), batch)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss_val = loss.item()\n",
    "        running_loss += loss_val\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('batch loss average: {}'.format(running_loss/(i+1)))\n",
    "    \n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config.n_output = 2\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss average: 0.7985257506370544\n",
      "0.44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-376-ace5c4285863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-375-b9c9dfeee783>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config, n_epoch, model, criterion, optimizer, data_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ReviewSentClf(config)\n",
    "model.to(config.device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_ls, score_ls = [], []\n",
    "best_epoch, best_loss, best_score = 0, 0, 0\n",
    "for epoch in range(n_epoch):\n",
    "    loss = train_model(config, epoch, model, criterion, optimizer, train_loader)\n",
    "    score = eval_model(config, model, test_loader)\n",
    "    loss_ls.append(loss)\n",
    "    score_ls.append(score)\n",
    "    \n",
    "    if best_score < score:\n",
    "        best_epoch, best_loss, best_score = epoch, loss, score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
