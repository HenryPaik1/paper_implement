{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_file = '/home/henry/Documents/wrapper/transformer-evolution/transformer/kowiki.model'\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. vocab\n",
    "- Word Piece Model\n",
    "    - 단어와 subword unit으로 나눔\n",
    "    - 단어의 시작을 특수문자; subword unit은 space로 나눔;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/home/henry/Documents/wrapper/transformer-evolution/ratings_train.txt'\n",
    "# lines = []; outputs = [];\n",
    "# with open(data_dir, \"r\") as f:\n",
    "#     for idx, line in enumerate(f):\n",
    "#         if idx == 0:\n",
    "#             continue\n",
    "#         if idx == 4:\n",
    "#             break\n",
    "#         temp = line.split()\n",
    "#         lines.append(' '.join(temp[1:-1]))\n",
    "#         outputs.append(temp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "  \"겨울은 추워요.\",\n",
    "  \"감기 조심하세요.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁겨울', '은', '▁추', '워', '요', '.']\n",
      "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "for line in lines:\n",
    "    pieces = vocab.encode_as_pieces(line)\n",
    "    ids = vocab.encode_as_ids(line)\n",
    "    inputs.append(torch.tensor(ids))\n",
    "    print(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3091, 3604,  206, 3958, 3760, 3590]), tensor([ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590])]\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3091, 3604,  206, 3958, 3760, 3590,    0,    0],\n",
      "        [ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590]]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "print(inputs, inputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. embedding\n",
    "\n",
    "### 2.1. word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)\n",
    "d_hidn = 128\n",
    "nn_emb = nn.Embedding(num_embeddings=n_vocab, embedding_dim=d_hidn)\n",
    "\n",
    "# torch.Size([2, 8, 128])\n",
    "input_embs = nn_emb(inputs) # 임베딩 레이어에 input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. position embedding\n",
    "- $PE(pos, 2i) = sin(\\frac{pos}{1E+4^{2i / d_{model}}}),$\n",
    "- $PE(pos, 2i+1) = cos(\\frac{pos}{1E+4^{2i / d_{model}}}),$\n",
    "- i: embedding vector에서 idx\n",
    "- pos: sentence에서 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    \n",
    "    def _cal_angle(pos, i_hidn):\n",
    "        return pos / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    \n",
    "    def _get_posi_angle_vec(pos):\n",
    "        # sentence의 pos 하나(즉 단어 하나)에 대해 embedding; feature1, feature2, ...., feature128 \n",
    "        # 즉, sequence의 각 position을 128차원으로 position embedding\n",
    "        # 예컨대, 첫번째 자리 pos embedding은 정해져 있으므로, 첫번째 자리에 어떤 단어가 들어오든, \n",
    "        # 해당 pos embedding이 word embedding에 더해짐\n",
    "        return [_cal_angle(pos, i_hidn) for i_hidn in range(d_hidn)]\n",
    "    \n",
    "    # seq_pos: Word의 sentence 내의 위치 = i\n",
    "    sinusoid_table = np.array([_get_posi_angle_vec(seq_pos) for seq_pos in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 0::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128)\n"
     ]
    }
   ],
   "source": [
    "# 64개 단어가 하나의 sentence; 128차원 posion임베딩;\n",
    "n_seq = 64\n",
    "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# row: sentence에서 위치(pos)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.pcolormesh(pos_encoding, cmap='twilight_shifted')\n",
    "plt.xlabel('embd_feature')\n",
    "plt.xlim([0, d_hidn])\n",
    "plt.ylabel('word_pos')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. pos embd + word embd\n",
    "- 의문: pos embd도 train 필요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = torch.FloatTensor(pos_encoding)\n",
    "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze=True) # layer; lookup table\n",
    "# >> Embedding(64, 128)\n",
    "\n",
    "position = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype)\n",
    "# >> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "position = position.expand(inputs.size())\n",
    "# >> tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "#            [0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "position = position.contiguous() + 1\n",
    "# >> tensor([[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "#            [1, 2, 3, 4, 5, 6, 7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask = inputs.eq(0) # find elems equal to 0 = pad\n",
    "# tensor([[False, False, False, False, False, False,  True,  True],\n",
    "#         [False, False, False, False, False, False, False, False]])\n",
    "\n",
    "position.masked_fill_(pos_mask, 0) # pad는 position 정보도 주지 않음\n",
    "# tensor([[1, 2, 3, 4, 5, 6, 0, 0],\n",
    "#         [1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "pos_embs = nn_pos(position)\n",
    "# pos_embs.shape \n",
    "# >> torch.Size([2, 8, 128]) # batch_size, sequence_len, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3091, 3604,  206, 3958, 3760, 3590,    0,    0],\n",
      "        [ 212, 3605,   53, 3832, 3596, 3682, 3760, 3590]])\n",
      "tensor([[1, 2, 3, 4, 5, 6, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 6, 7, 8]])\n",
      "torch.Size([2, 8, 128])\n",
      "torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "print(inputs) # embeddimg idx coresponding to the word\n",
    "print(position)\n",
    "print(pos_embs.size())\n",
    "\n",
    "input_vector = input_embs + pos_embs\n",
    "print(input_vector.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(1,10, (100,))\n",
    "b = np.random.randint(1,10, (100,))\n",
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scale Dot Product Attention\n",
    "$$\n",
    "Attention(Q, K, V) = softmax_k(\\dfrac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "- 왜 scale? \n",
    "    - d_k는 key vector의 dim. \n",
    "    - depth가 깊다면, $QK^T$ 값이 커짐\n",
    "> \"The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\"\n",
    "(https://www.tensorflow.org/tutorials/text/transformer)\n",
    "\n",
    "- detail: 내적은 두 벡터의 크기 * 둘 간의 cosin angle\n",
    "    - $cos \\theta = \\dfrac{A \\dot B}{|A||B|}$\n",
    "    - key dim으로 정규화하면 값이 너무 커지는 것을 방지할 수 있음(https://physics.stackexchange.com/questions/252086/dot-product-approaches-zero-as-the-magnitude-of-the-vectors-increase)\n",
    "- key, query, value? \n",
    "    - 기존 attention 모델은, decoder time step t의 hidden state와 encoder의 모든 time step의 hidden state간의 score를 산출. \n",
    "    - 그리고 그 score를 다시 encoder의 hidden state에 곱하여 weight를 주고, 합산하여 context vector를 산출. \n",
    "    - 위 context vector는, time step t의 hidden state와 concat.\n",
    "    - concat된 vector는 feedforward 통과하여 time step t의 출력값\n",
    "    - 이를 transformer와 비교해보자면, score은 **value**(but value vector 따로 있음), decoder의 hidden state는 **query**, encoder의 hidden state는 **key**에 해당.\n",
    "    \n",
    ">  \"Traditionally, the attention weights were the relevance of the encoder hidden states (values) in processing the decoder state (query) and were calculated based on the encoder hidden states (keys) and the decoder hidden state (query).\"(https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = input_vector\n",
    "K = input_vector\n",
    "V = input_vector\n",
    "inputs.eq(0) # 0과 동일한 indice\n",
    "inputs.eq(0).unsqueeze(1) # dim에 1차원 추가\n",
    "# >> torch.Size([2, 1, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.eq(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attn_mask: key padding indice 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False, False, False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False, False, False, False, False]]])\n",
      "tensor([[[False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False],\n",
      "         [False, False, False, False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), Q.size(1))\n",
    "# >> torch.Size([2, 8, 8])\n",
    "print(inputs.eq(0).unsqueeze(1))\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q.k^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "tensor([[195.2584,  56.8679,  44.3942,  62.4791,  53.6402,  66.9539,  57.5793,\n",
      "          57.5793],\n",
      "        [ 56.8679, 199.4895,  71.4400,  50.1359,  74.5325,  83.2698,  33.6057,\n",
      "          33.6057],\n",
      "        [ 44.3942,  71.4400, 194.6201,  87.3249, 113.1396, 118.8177,  45.8072,\n",
      "          45.8072],\n",
      "        [ 62.4791,  50.1359,  87.3249, 234.3755, 133.8546, 134.7569,  38.9334,\n",
      "          38.9334],\n",
      "        [ 53.6402,  74.5325, 113.1396, 133.8546, 277.1770, 153.3280,  13.3131,\n",
      "          13.3131],\n",
      "        [ 66.9539,  83.2698, 118.8177, 134.7569, 153.3280, 283.6367,  48.7106,\n",
      "          48.7106],\n",
      "        [ 57.5793,  33.6057,  45.8072,  38.9334,  13.3131,  48.7106, 208.7577,\n",
      "         208.7577],\n",
      "        [ 57.5793,  33.6057,  45.8072,  38.9334,  13.3131,  48.7106, 208.7577,\n",
      "         208.7577]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. scale\n",
    "- $1/\\sqrt{d_k}$\n",
    "- d_head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_head = 64\n",
    "scores = scores.mul(1/d_head**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 mask\n",
    "- score matrix\n",
    "    - col: key\n",
    "    - row: query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4407e+01,  7.1085e+00,  5.5493e+00,  7.8099e+00,  6.7050e+00,\n",
       "           8.3692e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.1085e+00,  2.4936e+01,  8.9300e+00,  6.2670e+00,  9.3166e+00,\n",
       "           1.0409e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 5.5493e+00,  8.9300e+00,  2.4328e+01,  1.0916e+01,  1.4142e+01,\n",
       "           1.4852e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.8099e+00,  6.2670e+00,  1.0916e+01,  2.9297e+01,  1.6732e+01,\n",
       "           1.6845e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 6.7050e+00,  9.3166e+00,  1.4142e+01,  1.6732e+01,  3.4647e+01,\n",
       "           1.9166e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 8.3692e+00,  1.0409e+01,  1.4852e+01,  1.6845e+01,  1.9166e+01,\n",
       "           3.5455e+01, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.1974e+00,  4.2007e+00,  5.7259e+00,  4.8667e+00,  1.6641e+00,\n",
       "           6.0888e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 7.1974e+00,  4.2007e+00,  5.7259e+00,  4.8667e+00,  1.6641e+00,\n",
       "           6.0888e+00, -1.0000e+09, -1.0000e+09]],\n",
       "\n",
       "        [[ 2.1984e+01,  7.4406e+00,  1.0338e+01,  6.4771e+00,  8.1974e+00,\n",
       "           7.9884e+00,  9.3757e+00,  7.2515e+00],\n",
       "         [ 7.4406e+00,  2.5475e+01,  1.0182e+01,  1.1326e+01,  1.2343e+01,\n",
       "           1.1618e+01,  1.3484e+01,  1.7287e+01],\n",
       "         [ 1.0338e+01,  1.0182e+01,  3.2733e+01,  1.0402e+01,  1.3615e+01,\n",
       "           1.1347e+01,  1.6779e+01,  1.6683e+01],\n",
       "         [ 6.4771e+00,  1.1326e+01,  1.0402e+01,  2.6711e+01,  1.5452e+01,\n",
       "           1.5909e+01,  1.8421e+01,  2.2145e+01],\n",
       "         [ 8.1974e+00,  1.2343e+01,  1.3615e+01,  1.5452e+01,  3.4309e+01,\n",
       "           2.2486e+01,  2.0667e+01,  2.4438e+01],\n",
       "         [ 7.9884e+00,  1.1618e+01,  1.1347e+01,  1.5909e+01,  2.2486e+01,\n",
       "           3.9289e+01,  2.8577e+01,  2.9457e+01],\n",
       "         [ 9.3757e+00,  1.3484e+01,  1.6779e+01,  1.8421e+01,  2.0667e+01,\n",
       "           2.8577e+01,  4.7835e+01,  3.3263e+01],\n",
       "         [ 7.2515e+00,  1.7287e+01,  1.6683e+01,  2.2145e+01,  2.4438e+01,\n",
       "           2.9457e+01,  3.3263e+01,  4.9091e+01]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill_(attn_mask, -1e9)\n",
    "# torch.Size([2, 8, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. softmax\n",
    "- https://stackoverflow.com/questions/49036993/pytorch-softmax-what-dimension-to-use\n",
    "- https://blog.csdn.net/qq_36097393/article/details/89319643\n",
    "- dim=-1이면 가장 높은 차원 기준 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "attn_prob = nn.Softmax(dim=-1)(scores) # dim 유지\n",
    "print(attn_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(size=(2,2,3,4,5))\n",
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=-1)(data).sum(dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. attn_prov * V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "context = torch.matmul(attn_prob, V)\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-Head attn\n",
    "- d_hidn = embedding layer 아웃풋의 dim\n",
    "    - embedding dim\n",
    "- d_head = Q, K, V의 dim\n",
    "    - embedding된 input값이\n",
    "- d_hidn > d_head: 연산량 감소하기 위함 eg. 128 > 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = input_vector\n",
    "K = input_vector\n",
    "V = input_vector\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input을 multi head로 복제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = Q.size(0)\n",
    "d_hidn = 128 # embedding dim\n",
    "d_head = 64 # K, Q, V의 dim\n",
    "n_head = 2\n",
    "\n",
    "# linear feed forward layer\n",
    "W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_V = nn.Linear(d_hidn, n_head * d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. multi-head K, Q, V input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 8, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bs, n_seq, n_head, d_head\n",
    "# torch.Size([2, 8, 2, 64])\n",
    "W_Q(Q).view(batch_size, -1, n_head, d_head).shape\n",
    "# torch.Size([2, 2, 8, 64]) \n",
    "W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 8, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (bs, n_head, n_seq, d_head)\n",
    "Qs = W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
    "Ks = W_K(K).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
    "Vs = W_V(V).view(batch_size, -1, n_head, d_head).transpose(1, 2)\n",
    "Vs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. multi-head attn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([2, 8, 8]) -> torch.size([2, 2, 8, 8])\n",
    "attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.Size([2, 2, 8, 64]) * torch.Size([2, 2, 64, 8])\n",
    "# output: torch.Size([2, 2, 8, 8])\n",
    "scores = torch.matmul(Qs, Ks.transpose(-1, -2))\n",
    "scores = scores.masked_fill(attn_mask, -np.inf)\n",
    "\n",
    "# bs, n_head, [n_q_seq, n_k_seq]\n",
    "# torch.Size([2, 2, 8, 8])\n",
    "attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "# bs, n_head, n_q_seq, d_v\n",
    "# torch.Size([2, 2, 8, 64])\n",
    "context = torch.matmul(attn_prob, Vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose: bs, n_q_seq, n_head, d_v\n",
    "# view: bs, n_q_seq, n_head * d_v\n",
    "# -> torch.Size([2, 8, 128])\n",
    "context = context\\\n",
    ".transpose(1, 2)\\\n",
    ".contiguous()\\\n",
    ".view(batch_size, -1, n_head * d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([2, 8, 128])\n",
    "# bs, [n_q_seq, n_hidn] <- embedding된 input size와 동일\n",
    "enc_output = nn.Linear(n_head * d_head, d_hidn)(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. feedforward\n",
    "### 6.1. 1st linear feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels=d_hidn, \\\n",
    "                  out_channels=d_hidn * 4, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs, [n_hidn, n_q_seq]\n",
    "# torch.Size([2, 128, 8])\n",
    "enc_output.transpose(1,2).shape\n",
    "\n",
    "# torch.Size([2, 512, 8])\n",
    "enc_output = conv1(enc_output.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output = F.gelu(enc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. 2nd linear feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.Conv1d(in_channels=d_hidn * 4, \\\n",
    "                  out_channels=d_hidn, \\\n",
    "                  kernel_size=1)\n",
    "# torch.Size([2, 128, 8])\n",
    "enc_output = conv2(enc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'n_output': 2}\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "config = dict({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12,\n",
    "    \"n_output\": 2\n",
    "})\n",
    "print(config)\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norcm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm.epsilon)\n",
    "        \n",
    "        self.pos_feedforward = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
    "        # MultiHeadAttention: forward(self, Q, K, V, attn_mask)\n",
    "        \n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
    "        self_attn_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask) \n",
    "        self_attn_outputs = self.layer_norm1(dec_inputs + self_attn_outputs) # LaterNorm(residual)\n",
    "        \n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_attn_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_attn_outputs = self.layer_norm2(self_attn_outputs + dec_enc_attn_outputs) # LaterNorm(residual)\n",
    "        \n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_feedforward(dec_enc_attn_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_attn_outputs + ffn_outputs)\n",
    "        \n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        position = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype)\\\n",
    "                        .expands(dec_inputs.size(0), dec_inputs.size(1)).contiguouse() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "        \n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "        \n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad) # get_attn_pad_mask(seq_q, seq_k, i_pad)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs) # 대각선 기준 위 mask\n",
    "        \n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "        \n",
    "        self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, d_dec_seq, n_enc_seq)\n",
    "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "        return dec_outputs, self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        \n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "        \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        # (bs, n_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # (bs, n_dec_seq, n_dec_vocab), (bs, n_head, n_enc_seq, n_enc_seq), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Naver movie clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieClf(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = Transformer(self.config)\n",
    "        self.projection = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "        \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        dec_outputs, ence_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\n",
    "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\n",
    "        logit = self.projection(dec_outputs)\n",
    "        \n",
    "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋\n",
    "    - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'ratings_test.txt'\n",
    "data = []\n",
    "with open(fn) as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: \n",
    "            break\n",
    "        _, t, l = line.split('\\t')\n",
    "        l = l.split('\\n')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "sentence = []\n",
    "d = []\n",
    "\n",
    "line_cnt = 0\n",
    "fn = 'ratings_test.txt'\n",
    "data = []\n",
    "with open(fn) as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        d.append(line)\n",
    "        line_cnt += 1\n",
    "        if line_cnt == 1:\n",
    "            continue\n",
    "        if not line: \n",
    "            break\n",
    "        _, t, l = line.split('\\t')\n",
    "        l = l.split('\\n')[0]\n",
    "        labels.append(int(l))\n",
    "        sentence.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        fn = 'ratings_test.txt'\n",
    "        with open(fn) as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                line_cnt += 1\n",
    "                if line_cnt == 1:\n",
    "                    continue\n",
    "                if not line: \n",
    "                    break\n",
    "                _, t, l = line.split('\\t')\n",
    "                l = l.split('\\n')[0]\n",
    "                self.labels.append(int(l))\n",
    "                self.sentences.append(t)\n",
    "        \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]), torch.tensor(self.sentences[item]), torch.tensor([self.vocab.piece_to_id(\"[BOS]\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(inputs):\n",
    "    labels, enc_inputs, dec_inputs = list(zip(*inputs))\n",
    "    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)\n",
    "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        enc_inputs,\n",
    "        dec_inputs\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "fn =  'ratings_test.txt'\n",
    "train_dataset = MovieDataset(vocab, fn)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "fn = 'ratings_test.txt'\n",
    "test_dataset = MovieDataset(vocab, fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. train\n",
    "- https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "    \n",
    "    n_word_total = 0\n",
    "    n_correct_total = 0\n",
    "    for i, value in enumerate(data_loader):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda x: x.to(config.device), value) # cpu/gpu tensor\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "        _, idx = logits.max(dim=1) # return val, idx\n",
    "        match = torch.eq(idx, labels).detach() # detach: \"Returns a new Tensor, detached from the current graph.\"\"\n",
    "        matchs.extend(match.cpu())\n",
    "        acc = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, epoch, model, criterion, optimizer, train_loader):\n",
    "    loss_ls = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, value in enumerate(train_loader):\n",
    "        labels, enc_inputs, dec_inputs = map(lambda x: x.to(config.device), value)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss_val= loss.item()\n",
    "        loss_ls.append(loss_val)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.mean(loss_ls) # batch 당 loss 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'types.SimpleNamespace' object has no attribute 'layer_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a17feba25821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovieClf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-f21a5a97c3a4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_hidn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-518b86dbf142>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c063c0f3b165>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msinusoid_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sinusoid_encoding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dec_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_hidn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinusoid_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c063c0f3b165>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msinusoid_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sinusoid_encoding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dec_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_hidn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinusoid_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-51460f8ba65f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_enc_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_hidn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_feedforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoswiseFeedForwardNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'types.SimpleNamespace' object has no attribute 'layer_norm'"
     ]
    }
   ],
   "source": [
    "model = MovieClf(config)\n",
    "model.to(config.device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_epoch, best_loss, best_score = 0, 0, 0\n",
    "loss_ls, score_ls = [], []\n",
    "for epoch in range(n_epoch):\n",
    "    loss = train_epoch(config, epoch, model, criterion, optimizer, train_loader)\n",
    "    score = eval_epoch(config, model, test_loader)\n",
    "    \n",
    "    loss_ls.append(loss)\n",
    "    scores.append(score)\n",
    "    \n",
    "    if best_score < score:\n",
    "        best_epoch, best_loss, best_score = epoch, loss, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_conda",
   "language": "python",
   "name": "torch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
