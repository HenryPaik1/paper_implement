{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference\n",
    "    - https://github.com/HMJiangGatech/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/pregenerate_training_data.py\n",
    "    - https://paul-hyun.github.io/bert-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "import sentencepiece as spm\n",
    "\n",
    "VOCAB_PATH = \"/home/henry/Documents/wrapper/source\"\n",
    "vocab_file = f\"{VOCAB_PATH}/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "config = dict({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_seg_type\": 2,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(d_ff=1024, d_head=64, d_hidn=256, dropout=0.1, i_pad=0, layer_norm_epsilon=1e-12, n_enc_seq=256, n_enc_vocab=8007, n_head=4, n_layer=6, n_seg_type=2)\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    outputs: [bs, len_seq, d_hidn] <- 잘 임베딩된 input seq\n",
    "    ointput_cls = outputs[:, 0].contiguous(): [bs, d_hidn]\n",
    "    - classification은 [cls] token의 임베딩만 사용\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(slef, inputs, segment):\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
    "        ointput_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls)\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'state_dict': self.state_dict()\n",
    "                   }, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save['state_dict'])\n",
    "        return save['epoch'], save['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTpretrain(nn.Module):\n",
    "    \"\"\"\n",
    "    self.feedforward_lm.weight\n",
    "    - transformer encoder의 pretrained embedding layer weight 사용\n",
    "    logits_cls: [bs, 2]\n",
    "    - binary classification\n",
    "    logits_lm: [bs, len_enc_seq, n_enc_vocab]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert  = BERT(self.config)\n",
    "        # cls\n",
    "        self.feedforward_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n",
    "        # lm\n",
    "        self.feedforward_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
    "        self.feedforward_lm.weight = self.bert.encoder.enc_emb.weight\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        logits_cls = self.feedforward_cls(outputs)\n",
    "        logits_lm = self.feedforward_lm(outputs)\n",
    "        return logits_cls, logits_lm, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "- https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/trainer/pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    masking subwords(15% of entire subwords)\n",
    "    - mask_cnt: len(subwords) * 0.15\n",
    "    - [MASK]: 80% of masking candidate token\n",
    "    - original token: 10% of masking candidate token\n",
    "    - another token: 10% of masking candidate token\n",
    "    \"\"\"\n",
    "    candidate_idx = []\n",
    "\n",
    "    ## subwords in the same list augment a sementic word \n",
    "    ## eg. [[0], [1], [2], [4, 5]] -> token_idx 4 + 5 is semantic word\n",
    "    # A list represent a sementic word\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "        if 0 < len(candidate_idx) and token.find(u'\\u2581') < 0: #  LOWER ONE EIGHTH BLOCK\n",
    "#        if 0 < len(candidate_idx) and token.find('_') < 0: #  test code\n",
    "            candidate_idx[-1].append(i)\n",
    "        else:\n",
    "            candidate_idx.append([i])\n",
    "    np.random.shuffle(candidate_idx)\n",
    "\n",
    "    mask_lms = []\n",
    "    for idx_set in candidate_idx:\n",
    "        # check if len(mask_lms) exceeds threshold\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(idx_set) > mask_cnt:\n",
    "            continue\n",
    "\n",
    "        ## masking subwords with 15% probability\n",
    "        ## mask_cnt is len(subwords) * 0.15 \n",
    "        # iter subwords idx\n",
    "        for sub_idx in idx_set:\n",
    "            masked_token = None\n",
    "\n",
    "            ### assign value to masked token: [MASK], original token, random token\n",
    "            # 80% of masking candidate are replaced with '[MASK]' token\n",
    "            if np.random.uniform() < 0.8:\n",
    "                masked_token = '[MASK]'\n",
    "            # remainng 20% of masking candidate\n",
    "            else:\n",
    "                # 10% of remaining preserve original token\n",
    "                if np.random.uniform() < 0.5:\n",
    "                    masked_token = tokens[sub_idx]\n",
    "                # 10% of ones are replaced with rnadom token    \n",
    "                else:\n",
    "                    masked_token = np.random.choice(vocab_list)\n",
    "\n",
    "                ### replace subword with masked_token value    \n",
    "                mask_lms.append({'idx': sub_idx, 'label':tokens[sub_idx]})\n",
    "                tokens[sub_idx] = masked_token\n",
    "                \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x['idx'])\n",
    "    mask_idx = [mask_dict['idx'] for mask_dict in mask_lms]\n",
    "    mask_label = [mask_dict['label'] for mask_dict in mask_lms]\n",
    "#     print(candidate_idx)\n",
    "#     print(mask_lms)\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_token(tokenA, tokenB, max_seq):\n",
    "    \"\"\"\n",
    "    truncate long sequence\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_len = len(tokenA) + len(tokenB)\n",
    "        if total_len <= max_seq:\n",
    "            break\n",
    "        if len(tokenA) > len(tokenB):\n",
    "            tokenA.pop()\n",
    "        else:\n",
    "            tokenB.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['_I am', '_on', '_it', '_so wh', 'at',]\n",
    "tokens = ['_I am', '_on', '_it', '[CLS]', '_so wh', 'at',]\n",
    "mask_cnt = int(len(tokens) * 0.15)+1\n",
    "create_pretrain_mask(tokens, mask_cnt, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# load data and make pre train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
    "    \"\"\"\n",
    "    read text and return train data set format\n",
    "    \"\"\"\n",
    "    vocab_list = []\n",
    "    for id_ in range(vocab.get_piece_size()):\n",
    "        if not vocab.is_unknown(id_):\n",
    "            vocab_list.append(vocab.id_to_piece(id_))\n",
    "    \n",
    "    sent_cnt = 0\n",
    "    with open(in_file, 'r') as in_f:\n",
    "        for sent in in_f:\n",
    "            sent_cnt += 1\n",
    "    \n",
    "    paragraph_ls = []\n",
    "    with open(in_file, 'r') as in_f:\n",
    "        paragraph = []\n",
    "        for i, sent in enumerate(in_f):\n",
    "            sent = sent.strip()\n",
    "            \n",
    "            ## blank means end of the paragraph\n",
    "            if sent == '':\n",
    "                # if not the beggining of the paragraph\n",
    "                # it is the end of the paragraph\n",
    "                if 0 < len(paragraph):\n",
    "                    paragraph_ls.append(paragraph)\n",
    "                    paragraph = [] # generate new paragraph list\n",
    "                    # check if exceeding 100 thaousand paragraphs\n",
    "                    if 1e+5 < len(paragraph_ls): \n",
    "                        break \n",
    "                        \n",
    "            ## subwords in list is part of semantic token\n",
    "            # eg. ['▁지','미','▁카','터']\n",
    "            else:\n",
    "                pieces = vocab.encode_as_pieces(sent)\n",
    "                if 0 < len(pieces):\n",
    "                    paragraph.append(pieces)\n",
    "        if paragraph:\n",
    "            paragraph_ls.append(paragraph)\n",
    "    \n",
    "    # masking def: create_pretrain_mask\n",
    "    for index in range(count):\n",
    "        output = out_file.format(index)\n",
    "        if os.path.isfile(output):\n",
    "            continue\n",
    "        with open(output, 'w') as out_f:\n",
    "            for i, paragraph in enumerate(paragraph_ls):\n",
    "                masking_info = create_pretrain_instances(paragraph_ls, i, paragraph, n_seq, mask_prob, vocab_list)\n",
    "                for elem in masking_info:\n",
    "                    out_f.write(json.dumps(elem))\n",
    "                    out_f.write('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki.txt'\n",
    "sentences = []\n",
    "with open(in_PATH, 'r') as in_f:\n",
    "        for i, sent in enumerate(in_f):\n",
    "            # '': paragraph delimiter\n",
    "            if i == 7:\n",
    "                sentences.append('')\n",
    "            if i == 13:\n",
    "                break\n",
    "            sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " ['지미 카터\\n',\n",
       "  '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\\n',\n",
       "  '지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n',\n",
       "  '1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_ls = []\n",
    "paragraph = []\n",
    "for i, sent in enumerate(sentences):\n",
    "    sent = sent.strip()\n",
    "    ## blank means end of paragraph\n",
    "    if sent == '':\n",
    "        # if not the beggining of sentence\n",
    "        # it is the end of the paragraph\n",
    "        # generate new doc list\n",
    "        if 0 < len(paragraph):\n",
    "            paragraph_ls.append(paragraph)\n",
    "            paragraph = []\n",
    "            # check if 100thaousand paragraph\n",
    "            if 1e+5 < len(paragraph_ls):\n",
    "                break\n",
    "                \n",
    "    ## subwords in list is part of semantic token\n",
    "    # eg. ['▁지','미','▁카','터']\n",
    "    else:\n",
    "        pieces = vocab.encode_as_pieces(sent)\n",
    "        if 0 < len(pieces):\n",
    "            paragraph.append(pieces)\n",
    "if paragraph:\n",
    "    paragraph_ls.append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of paragraph: 2\n"
     ]
    }
   ],
   "source": [
    "print('num of paragraph:', len(paragraph_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 1st sentence\n",
      "['▁지', '미', '▁카', '터']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 1st sentence')\n",
    "print(paragraph_ls[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 2nd sentence\n",
      "['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 2nd sentence')\n",
    "print(paragraph_ls[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd paragraph - 1st sentence\n",
      "['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('2nd paragraph - 1st sentence')\n",
    "print(paragraph_ls[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki.txt'\n",
    "out_PATH = '/home/henry/Documents/wrapper/source/out_kowiki.txt'\n",
    "\n",
    "count = 1\n",
    "n_seq = 256\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_PATH, out_PATH, count, n_seq, mask_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# pretrain dataset for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(paragraph_ls, paragraph_idx, paragraph, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    create NSP train set\n",
    "    \"\"\"\n",
    "    # 3 special token: [CLS], [SEP] for sent A, [SEP] for sent B\n",
    "    max_seq_len = n_seq - 2 - 1\n",
    "    target_seq_len = max_seq_len # [CLS], segA, segA, ..., [SEP], segB, ...\n",
    "\n",
    "    instances = []\n",
    "    temp_sentences = []\n",
    "    temp_sent_seq_length = 0\n",
    "    for i, sent in enumerate(paragraph):\n",
    "        ## A. not the last sentence of the paragraph\n",
    "        temp_sentences.append(sent)\n",
    "        temp_sent_seq_length += len(sent)\n",
    "\n",
    "        ## B. check if the last sentence of the paragraph\n",
    "        ## or current_length is longer than or equal to target_seq_len\n",
    "        if i == len(paragraph) - 1 or current_length >= target_seq_len:\n",
    "\n",
    "            if temp_sentences:\n",
    "                ## A. sentence A segment: from 0 to a_end\n",
    "                a_end = 1 # 1st sentence\n",
    "                if len(temp_sentences) != 1:\n",
    "                    a_end = np.random.randint(1, len(temp_sentences))\n",
    "                # append sentence of current_chunk \n",
    "                # from the front to the back\n",
    "                tokenA = []\n",
    "                for _, s in enumerate(temp_sentences[:a_end]):\n",
    "                    tokenA.extend(s)\n",
    "\n",
    "                ## B. sentence B segment\n",
    "                # A. Actual next\n",
    "                tokenB = []\n",
    "                if len(temp_sentences) > 1 and np.random.uniform() > 0.5:\n",
    "                    is_next = True\n",
    "                    for j in range(a_end, len(temp_sentences)):\n",
    "                        tokenB.extend(temp_sentences[j])\n",
    "\n",
    "                # B. random next\n",
    "                else:\n",
    "                    is_next = False\n",
    "                    tokenB_len = target_seq_len - len(tokenA)\n",
    "                    random_para_idx = para_idx\n",
    "                    while para_idx == random_para_idx:\n",
    "                        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "                    random_para = paragraph[random_para_idx]\n",
    "\n",
    "                    random_start = np.random.randint(0, len(random_para))\n",
    "                    for j in range(random_start, len(random_doc)):\n",
    "                        tokenB.extend(random_para[j])\n",
    "\n",
    "\n",
    "                truncate_token(tokenA, tokenB, max_seq_len)\n",
    "                assert 0 < len(tokenA)\n",
    "                assert 0 < len(tokenB)\n",
    "\n",
    "                token = ['[CLS]'] + tokenA + ['[SEP]'] + tokenB +['[SEP]']\n",
    "                segment = [0]*(len(tokenA)+2) + [1]*(len(tokenB)+1)\n",
    "\n",
    "                create_pretrain_mask(tokens, int((len(tokens)-3)*mask_prob), vocab_list)\n",
    "                instance = {\n",
    "                    \"tokens\": tokens,\n",
    "                    \"segment\": segment,\n",
    "                    \"is_next\": is_next,\n",
    "                    \"mask_idx\": mask_idx,\n",
    "                    \"mask_label\": mask_label\n",
    "                }\n",
    "                instances.append(instance)\n",
    "            temp_sentences = []\n",
    "            temp_sent_seq_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx]\n",
    "print(len(paragraph), paragraph[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "['▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤', '던', '▁대통령', '▁중', '▁하나', '다', '.', '▁인', '권', '▁문제', '와', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '▁한', '때', '▁한', '미', '▁관계', '가', '▁불', '편', '하기도', '▁했다', '.', '▁1978', '년', '▁대한민국', '에', '▁대한', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주', '한', '미', '군을', '▁철', '수', '하기로', '▁했다', '.', '▁그러나', '▁주', '한', '미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '▁반', '대에', '▁부', '딪', '혀', '▁주', '한', '미', '군은', '▁완전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '▁정', '권의', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '▁회복', '되었다', '.']\n",
      "['▁1979', '년', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '▁당시의', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '▁대해', '▁그는', '▁이', '▁사건', '으로', '▁큰', '▁충', '격을', '▁받았', '으며', ',', '▁사이', '러스', '▁밴', '스', '▁국', '무', '장', '관을', '▁조', '문', '사', '절', '로', '▁파견', '했다', '.', '▁12', '·', '12', '▁군사', '▁반란', '과', '▁5', '.', '17', '▁쿠', '데', '타', '에', '▁대해', '▁초기', '에는', '▁강', '하게', '▁비난', '했으나', ',', '▁미국', '▁정부', '가', '▁신', '군', '부를', '▁설', '득', '하는데', ',', '▁한', '계가', '▁있었고', '▁결국', '▁', '묵', '인', '하는', '▁', '듯', '한', '▁태', '도를', '▁보이', '게', '▁', '됐다', '.']\n",
      "['▁퇴', '임', '▁이후', '▁민간', '▁자', '원을', '▁적극', '▁활용', '한', '▁비', '영', '리', '▁기', '구', '인', '▁카', '터', '▁재', '단을', '▁설립', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '▁활동', '▁및', '▁기', '니', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(paragraph)):\n",
    "    print(paragraph[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_end 1\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "a_end 1\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "a_end 2\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "s ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "current_chunk[j] ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "a_end 3\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.'], ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "s ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "\n",
      "s ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "current_chunk[j] ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "current_chunk[j] ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "a_end 3\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.'], ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "s ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "\n",
      "s ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "current_chunk[j] ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "current_chunk[j] ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "a_end 3\n",
      "current_chunk[:a_end] [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.'], ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']]\n",
      "\n",
      "s ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "\n",
      "s ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "\n",
      "s ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "\n",
      "current_chunk[j] ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "current_chunk[j] ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n",
      "current_chunk[j] ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "temp_sentences = []\n",
    "temp_sent_seq_length = 0 # num of tokens\n",
    "max_num_tokens = 256\n",
    "target_seq_len = np.random.randint(2, max_num_tokens) # min len of tokens\n",
    "\n",
    "for i, sent in enumerate(paragraph):\n",
    "    ## A. not the last sentence of the paragraph\n",
    "    current_chunk.append(sent)\n",
    "    current_length += len(sent)\n",
    "\n",
    "    ## B. check if it is the last sentence of the paragraph\n",
    "    ## or current_length is longer than or equal to target_seq_len\n",
    "    if i == len(paragraph) - 1 or current_length >= target_seq_len:\n",
    "\n",
    "        if current_chunk:\n",
    "            ## A. sentence A segment: from 0 to a_end\n",
    "            a_end = 1\n",
    "            if len(current_chunk) != 1:\n",
    "                a_end = np.random.randint(1, len(current_chunk))\n",
    "            # append the sentences to tokenA \n",
    "            # from the front to the back\n",
    "            tokenA = []\n",
    "            for _, s in enumerate(current_chunk[:a_end]):\n",
    "                tokenA.extend(s)\n",
    "\n",
    "            ## B. sentence B segment\n",
    "            tokenB = []\n",
    "            # A. Actual next\n",
    "            if len(current_chunk) > 1 and np.random.uniform() >= 0.5:\n",
    "                is_next = True\n",
    "                for j in range(a_end, len(current_chunk)):\n",
    "                    tokenB.extend(current_chunk[j])\n",
    "            # B. random next\n",
    "            else:\n",
    "                is_next = False\n",
    "                tokenB_len = target_seq_len - len(tokenA)\n",
    "                random_para_idx = para_idx\n",
    "                while para_idx == random_para_idx:\n",
    "                    random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "                random_para = paragraph[random_para_idx]\n",
    "\n",
    "                random_start = np.random.randint(0, len(random_para))\n",
    "                for j in range(random_start, len(random_para)):\n",
    "                    tokenB.extend(random_para[j])\n",
    "            assert 0 < len(tokenA)\n",
    "            assert 0 < len(tokenB)\n",
    "            tokens = [\"[CLS]\"] + tokenA + [\"[SEP]\"] + tokenB + [\"[SEP]\"]\n",
    "            #print(tokens, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i = 0: start with 1st sentence\n",
    "- max_len_doc == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 256\n",
    "max_seq = n_seq - 3\n",
    "target_seq_len = max_seq\n",
    "\n",
    "instances = []\n",
    "temp_sentences = []\n",
    "temp_sent_seq_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "# of sentences in the paragraph: 6\n",
      "temp_sentences len: 1 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']]\n"
     ]
    }
   ],
   "source": [
    "# the case of 1st sentence in 2nd paragraph \n",
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx] # 2nd paragraph\n",
    "\n",
    "temp_sentences = []\n",
    "i = 0 \n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print(temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Is it the last sentence of the paragraph \n",
      "or current chunk longer than target_seq_len? True\n"
     ]
    }
   ],
   "source": [
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenA consists of 1 sentences \n",
      " ['▁퇴', '임', '▁이후', '▁민간', '▁자', '원을', '▁적극', '▁활용', '한', '▁비', '영', '리', '▁기', '구', '인', '▁카', '터', '▁재', '단을', '▁설립', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '▁활동', '▁및', '▁기', '니', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.']\n"
     ]
    }
   ],
   "source": [
    "if 0 < len(temp_sentences):\n",
    "    a_end = 1\n",
    "    if 1 < len(temp_sentences):\n",
    "        a_end = np.random.randint(1, len(temp_sentences))\n",
    "        \n",
    "    tokenA = []\n",
    "    for j in range(a_end):\n",
    "        tokenA.extend(temp_sentences[j]) # convert sentence to segment\n",
    "        \n",
    "print('tokenA consists of {} sentences'.format(a_end), '\\n', tokenA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB random sentence with 1/2 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenB avaliable length: 170\n",
      "tokenB len 1070 should be shorter than 170, \n",
      "or truncate segments\n",
      "['▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한']\n"
     ]
    }
   ],
   "source": [
    "tokensB = []\n",
    "if len(temp_sentences) == 1 or np.random.uniform() < 0.5:\n",
    "    is_next = False\n",
    "    tokenB_len = target_seq_len - len(tokenA)\n",
    "    print('tokenB avaliable length: {}'.format(tokenB_len))\n",
    "    \n",
    "    # choose sentence from other paragraph\n",
    "    random_para_idx = para_idx\n",
    "    while para_idx == random_para_idx:\n",
    "        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "    random_paragraph = paragraph_ls[random_para_idx]\n",
    "\n",
    "    random_start = np.random.randint(0, len(random_paratgraph))\n",
    "    for j in range(random_start, len(random_paragraph)):\n",
    "        tokenB.extend(random_paragraph[j])\n",
    "        if len(tokenB) > tokenB_len:\n",
    "            break\n",
    "print('tokenB len {} should be shorter than {}, \\nor truncate segments'.format(len(tokenB), tokenB_len))\n",
    "print(tokenB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB the sentence right after tokenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the end of the tokenA: \n",
      "['▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.']\n",
      "the beggining of the tokenB: \n",
      "['▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한']\n"
     ]
    }
   ],
   "source": [
    "is_next = 1\n",
    "for j in range(a_end, len(temp_sentences)):\n",
    "    tokenB.extend(temp_sentences[j])\n",
    "print('the end of the tokenA: \\n{}'.format(tokenA[-10:]))\n",
    "print('the beggining of the tokenB: \\n{}'.format(tokenB[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
