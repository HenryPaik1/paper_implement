{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference\n",
    "    - https://github.com/HMJiangGatech/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/pregenerate_training_data.py\n",
    "    - https://paul-hyun.github.io/bert-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, inspect\n",
    "import sys\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "from transformer.transformer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "import sentencepiece as spm\n",
    "\n",
    "VOCAB_PATH = \"/home/henry/Documents/wrapper/source\"\n",
    "vocab_file = f\"{VOCAB_PATH}/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "config = dict({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_seg_type\": 2,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "config = SimpleNamespace(**config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        attn_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        attn_outputs = self.layer_norm1(inputs + attn_outputs)\n",
    "        \n",
    "        ffn_outputs = self.pos_ffn(attn_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + attn_outputs)\n",
    "        \n",
    "        return ffn_outputs, attn_prob\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        self.pos_emb = nn.Embedding(self.config.n_enc_seq+1, self.config.d_hidn)\n",
    "        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "    def forward(self, inputs, segments):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype)\\\n",
    "            .expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "        \n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions) + self.seg_emb(segments)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "        \n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    outputs: [bs, len_seq, d_hidn] <- 잘 임베딩된 input seq\n",
    "    outputs_cls = outputs[:, 0].contiguous(): [bs, d_hidn]\n",
    "    - classification은 [cls] token의 임베딩만 사용\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
    "        outputs_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls)\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'state_dict': self.state_dict()\n",
    "                   }, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save['state_dict'])\n",
    "        return save['epoch'], save['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTpretrain(nn.Module):\n",
    "    \"\"\"\n",
    "    self.feedforward_lm.weight\n",
    "    - transformer encoder의 pretrained embedding layer weight 사용\n",
    "    logits_cls: [bs, 2]\n",
    "    - binary classification\n",
    "    logits_lm: [bs, len_enc_seq, n_enc_vocab]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert  = BERT(self.config)\n",
    "        # cls\n",
    "        self.feedforward_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n",
    "        # lm\n",
    "        self.feedforward_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
    "        self.feedforward_lm.weight = self.bert.encoder.enc_emb.weight\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        logits_cls = self.feedforward_cls(outputs_cls)\n",
    "        logits_lm = self.feedforward_lm(outputs)\n",
    "        return logits_cls, logits_lm, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Order of Pretrain func call\n",
    "- `make_pretrain_data` -> `create_pretrain_instances` -> `create_pretrain_mask`\n",
    "---\n",
    "# Masking\n",
    "- https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/trainer/pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    masking subwords(15% of entire subwords)\n",
    "    - mask_cnt: len(subwords) * 0.15\n",
    "    - [MASK]: 80% of masking candidate token\n",
    "    - original token: 10% of masking candidate token\n",
    "    - another token: 10% of masking candidate token\n",
    "    \"\"\"\n",
    "    candidate_idx = []\n",
    "\n",
    "    ## subwords in the same list augment a sementic word \n",
    "    ## eg. [[0], [1], [2], [4, 5]] -> token_idx 4 + 5 is semantic word\n",
    "    # A list represent a sementic word\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "        if 0 < len(candidate_idx) and token.find(u'\\u2581') < 0: #  LOWER ONE EIGHTH BLOCK\n",
    "#        if 0 < len(candidate_idx) and token.find('_') < 0: #  test code\n",
    "            candidate_idx[-1].append(i)\n",
    "        else:\n",
    "            candidate_idx.append([i])\n",
    "    np.random.shuffle(candidate_idx)\n",
    "\n",
    "    mask_lms = []\n",
    "    for idx_set in candidate_idx:\n",
    "        # check if len(mask_lms) exceeds threshold\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(idx_set) > mask_cnt:\n",
    "            continue\n",
    "\n",
    "        ## masking subwords with 15% probability\n",
    "        ## mask_cnt is len(subwords) * 0.15 \n",
    "        # iter subwords idx\n",
    "        for sub_idx in idx_set:\n",
    "            masked_token = None\n",
    "\n",
    "            ### assign value to masked token: [MASK], original token, random token\n",
    "            # 80% of masking candidate are replaced with '[MASK]' token\n",
    "            if np.random.uniform() < 0.8:\n",
    "                masked_token = '[MASK]'\n",
    "            # remainng 20% of masking candidate\n",
    "            else:\n",
    "                # 10% of remaining preserve original token\n",
    "                if np.random.uniform() < 0.5:\n",
    "                    masked_token = tokens[sub_idx]\n",
    "                # 10% of ones are replaced with rnadom token    \n",
    "                else:\n",
    "                    masked_token = np.random.choice(vocab_list)\n",
    "\n",
    "                ### replace subword with masked_token value    \n",
    "                mask_lms.append({'idx': sub_idx, 'label':tokens[sub_idx]})\n",
    "                tokens[sub_idx] = masked_token\n",
    "                \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x['idx'])\n",
    "    mask_idx = [mask_dict['idx'] for mask_dict in mask_lms]\n",
    "    mask_label = [mask_dict['label'] for mask_dict in mask_lms]\n",
    "#     print(candidate_idx)\n",
    "#     print(mask_lms)\n",
    "    print(mask_idx, mask_label)\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_token(tokenA, tokenB, max_seq):\n",
    "    \"\"\"\n",
    "    truncate long sequence\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_len = len(tokenA) + len(tokenB)\n",
    "        print('max token {}\\ntotal_len {} = {} + {}'.format(max_seq, total_len, len(tokenA), len(tokenB)))\n",
    "        if total_len <= max_seq:\n",
    "            break\n",
    "        if len(tokenA) > len(tokenB):\n",
    "            tokenA.pop()\n",
    "        else:\n",
    "            tokenB.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_I am', '_on', '_it', '[CLS]', '_so wh', 'at'], [], [])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = ['_I am', '_on', '_it', '_so wh', 'at',]\n",
    "tokens = ['_I am', '_on', '_it', '[CLS]', '_so wh', 'at',]\n",
    "mask_cnt = int(len(tokens) * 0.15)+1\n",
    "create_pretrain_mask(tokens, mask_cnt, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# pretrain dataset for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(paragraph_ls, paragraph_idx, paragraph, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    create NSP train set\n",
    "    \"\"\"\n",
    "    # 3 special token: [CLS], [SEP] for sent A, [SEP] for sent B\n",
    "    max_seq_len = n_seq - 2 - 1\n",
    "    target_seq_len = max_seq_len # [CLS], segmentA, segmentA, ..., [SEP], segmentB, segmentB, ...\n",
    "\n",
    "    instances = []\n",
    "    temp_sentence = []\n",
    "    temp_sent_seq_length = 0 # num of tokens\n",
    "\n",
    "    max_num_tokens = 256\n",
    "    target_seq_len = np.random.randint(2, max_num_tokens) # min len of tokens\n",
    "    for i, sent in enumerate(paragraph):\n",
    "        ## A. not the last sentence of the paragraph\n",
    "        temp_sentence.append(sent)\n",
    "        temp_sent_seq_length += len(sent)\n",
    "\n",
    "        ## B. check if it is the last sentence of the paragraph\n",
    "        ## or temp_sent_seq_length is longer than or equal to target_seq_len\n",
    "        if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "            if temp_sentence:\n",
    "                ## A. sentence A segment: from 0 to a_end\n",
    "                a_end = 1\n",
    "                if len(temp_sentence) != 1:\n",
    "                    a_end = np.random.randint(1, len(temp_sentence))\n",
    "                # append the sentences to tokenA \n",
    "                # from the front to the back\n",
    "                tokenA = []\n",
    "                for _, s in enumerate(temp_sentence[:a_end]):\n",
    "                    tokenA.extend(s)\n",
    "\n",
    "                ## B. sentence B segment\n",
    "                tokenB = []\n",
    "                # A. Actual next\n",
    "                # is_next will be the label for NSP pretrain\n",
    "                if len(temp_sentence) > 1 and np.random.uniform() >= 0.5:\n",
    "                    is_next = True\n",
    "                    for j in range(a_end, len(temp_sentence)):\n",
    "                        tokenB.extend(temp_sentence[j])\n",
    "                # B. random next\n",
    "                else:\n",
    "                    is_next = False\n",
    "                    tokenB_len = target_seq_len - len(tokenA)\n",
    "                    random_para_idx = para_idx\n",
    "                    while para_idx == random_para_idx:\n",
    "                        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "                    random_para = paragraph[random_para_idx]\n",
    "\n",
    "                    random_start = np.random.randint(0, len(random_para))\n",
    "                    for j in range(random_start, len(random_para)):\n",
    "                        tokenB.extend(random_para[j])\n",
    "\n",
    "                truncate_token(tokenA, tokenB, max_seq)\n",
    "                assert 0 < len(tokenA)\n",
    "                assert 0 < len(tokenB)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokenA + [\"[SEP]\"] + tokenB + [\"[SEP]\"]\n",
    "                segment = [0]*(len(tokenA)  + 2) + [1]*(len(tokenB) + 1)\n",
    "                \n",
    "                tokens, mask_idx, mask_label = \\\n",
    "                    create_pretrain_mask(tokens, int((len(tokens)-3)*mask_prob), vocab_list)\n",
    "                instance = {\n",
    "                    'tokens': tokens,\n",
    "                    'segment': segment,\n",
    "                    'is_next': is_next,\n",
    "                    'mask_idx': mask_idx,\n",
    "                    'mask_label': mask_label\n",
    "                }\n",
    "\n",
    "                instances.append(instance)\n",
    "\n",
    "            # reset segment candidate\n",
    "            temp_sentence = []\n",
    "            temp_sent_seq_length = 0\n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx]\n",
    "print(len(paragraph), paragraph[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 0\n",
    "    - add 1st sequence to meet target sequence length\n",
    "    - max_len_doc == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target seq len: 253\n"
     ]
    }
   ],
   "source": [
    "n_seq = 256\n",
    "max_seq = n_seq - 3\n",
    "target_seq_len = max_seq\n",
    "\n",
    "instances = []\n",
    "temp_sentences = []\n",
    "temp_sent_seq_length = 0\n",
    "print('target seq len:', target_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 50\n",
      "# of sentences in the paragraph: 2\n",
      "temp_sentences len: 1 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']]\n"
     ]
    }
   ],
   "source": [
    "# the case of 1st sentence in 2nd paragraph \n",
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx] # 2nd paragraph\n",
    "\n",
    "temp_sentences = []\n",
    "i = 0 \n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 1\n",
    "    - add one more sentece to segments A to meet target sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 87\n",
      "# of sentences in the paragraph: 2\n",
      "temp_sentences len: 2 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']]\n",
      "Is it the last sentence of the paragraph \n",
      "or current chunk longer than target_seq_len? True\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "# temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "# print('current seq length:', temp_sent_seq_length)\n",
    "# print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "# print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "# if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "#     print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "# temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "# print('current seq length:', temp_sent_seq_length)\n",
    "# print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "# print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "# if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "#     print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenA consists of 1 sentences \n",
      " ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "if 0 < len(temp_sentences):\n",
    "    a_end = 1\n",
    "    if 1 < len(temp_sentences):\n",
    "        a_end = np.random.randint(1, len(temp_sentences))\n",
    "        \n",
    "    tokenA = []\n",
    "    for j in range(a_end):\n",
    "        tokenA.extend(temp_sentences[j]) # convert sentence to segment\n",
    "        \n",
    "print('tokenA consists of {} sentences'.format(a_end), '\\n', tokenA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB random sentence with 1/2 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenB avaliable length: 203\n",
      "tokenB len 198 should be longer than 203, \n",
      "or truncate segments\n",
      "['▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '▁의원', '▁선거']\n"
     ]
    }
   ],
   "source": [
    "tokenB = []\n",
    "if len(temp_sentences) == 1 or np.random.uniform() < 0.5:\n",
    "    is_next = False\n",
    "    tokenB_len = target_seq_len - len(tokenA) # minumum len of tokenB\n",
    "    print('tokenB avaliable length: {}'.format(tokenB_len))\n",
    "    \n",
    "    # choose sentence from other paragraph\n",
    "    random_para_idx = para_idx\n",
    "    while para_idx == random_para_idx:\n",
    "        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "    # this is random paragraph    \n",
    "    random_paragraph = paragraph_ls[random_para_idx]\n",
    "    \n",
    "    # add a series of sentences from random paragragraph\n",
    "    random_start = np.random.randint(0, len(random_paragraph))\n",
    "    for j in range(random_start, len(random_paragraph)):\n",
    "        tokenB.extend(random_paragraph[j])\n",
    "        if len(tokenB) > tokenB_len:\n",
    "            break\n",
    "print('tokenB len {} should be longer than {}, \\nor truncate segments'.format(len(tokenB), tokenB_len))\n",
    "print(tokenB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB the sentences right after tokenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the end of the tokenA: \n",
      "['▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n",
      "the beggining of the tokenB: \n",
      "['▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '▁의원', '▁선거']\n"
     ]
    }
   ],
   "source": [
    "is_next = 1\n",
    "for j in range(a_end, len(temp_sentences)):\n",
    "    tokenB.extend(temp_sentences[j])\n",
    "print('the end of the tokenA: \\n{}'.format(tokenA[-10:]))\n",
    "print('the beggining of the tokenB: \\n{}'.format(tokenB[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '[SEP]', '▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '▁당선', '되었다', '.', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '▁반', '대로', '▁무', '산', '되었다', '.', '▁카', '터', '는', '▁이집', '트', '와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠', '프', '▁데이', '비', '드', '에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"[CLS]\"] + tokenA + [\"[SEP]\"] + tokenB + [\"[SEP]\"]\n",
    "segment = [0]*(len(tokenA)  + 2) + [1]*(len(tokenB) + 1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, mask_idx, mask_label = \\\n",
    "    create_pretrain_mask(tokens, int((len(tokens)-3)*mask_prob), ['test_mask_voc1', 'test_mask_voc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁미국의 7\n",
      "인 9\n",
      "test_mask_voc1 37\n",
      "▁제한 44\n",
      "▁협 45\n",
      "상에 46\n",
      "▁조 47\n",
      "인 48\n",
      "▁196 52\n",
      "▁의원 60\n",
      "▁선거 61\n",
      "▁ 71\n",
      "▁입 74\n",
      "아 85\n",
      "년 95\n",
      "이 105\n",
      "▁지 127\n",
      "▁근무 129\n",
      "내 138\n",
      "▁내 149\n",
      ". 152\n",
      "▁1976 153\n",
      "년 154\n",
      "test_mask_voc2 159\n",
      "로 161\n",
      "워 172\n",
      "지 186\n",
      "을 188\n",
      "대로 196\n",
      "는 203\n",
      "▁이스라엘 207\n",
      "test_mask_voc2 215\n",
      "다 222\n",
      "트 223\n",
      "▁대통령 224\n",
      "동 235\n",
      "▁1970 253\n",
      "test_mask_voc2 272\n",
      ", 274\n",
      "해서 278\n",
      "test_mask_voc2 285\n"
     ]
    }
   ],
   "source": [
    "for label, idx in zip(mask_label, mask_idx):\n",
    "    print(label, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- truncate long segmentsA, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token 253\n",
      "total_len 253 = 127 + 126\n"
     ]
    }
   ],
   "source": [
    "truncate_token(tokenA, tokenB, max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < len(tokenA)\n",
    "assert 0 < len(tokenB)\n",
    "tokens = ['[CLS]'] + tokenA + ['[SEP]'] + tokenB + ['[SEP]']\n",
    "segment = [0]*(len(tokenA)  + 2) + [1]*(len(tokenB) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# make pre train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
    "    \"\"\"\n",
    "    read text and return train data set format\n",
    "    \"\"\"\n",
    "    vocab_list = []\n",
    "    for id_ in range(vocab.get_piece_size()):\n",
    "        if not vocab.is_unknown(id_):\n",
    "            vocab_list.append(vocab.id_to_piece(id_))\n",
    "    \n",
    "    paragraph_ls = []\n",
    "    with open(in_file, 'r') as in_f:\n",
    "        paragraph = []\n",
    "        for i, sent in enumerate(in_f):\n",
    "            sent = sent.strip()\n",
    "            \n",
    "            ## blank means end of the paragraph\n",
    "            if sent == '':\n",
    "                # if not the beggining of the paragraph\n",
    "                # it is the end of the paragraph\n",
    "                if 0 < len(paragraph):\n",
    "                    paragraph_ls.append(paragraph)\n",
    "                    paragraph = [] # generate new paragraph list\n",
    "                    # check if exceeding 100 thaousand paragraphs\n",
    "                    if 1e+5 < len(paragraph_ls): \n",
    "                        break \n",
    "                        \n",
    "            ## subwords in list is part of semantic token\n",
    "            # eg. ['▁지','미','▁카','터']\n",
    "            else:\n",
    "                pieces = vocab.encode_as_pieces(sent)\n",
    "                if 0 < len(pieces):\n",
    "                    paragraph.append(pieces)\n",
    "        if paragraph:\n",
    "            paragraph_ls.append(paragraph)\n",
    "    # masking def: create_pretrain_mask\n",
    "    for index in range(count):\n",
    "        output = out_file.format(index)\n",
    "#         if os.path.isfile(output):\n",
    "#             continue\n",
    "        with open(output, 'w') as out_f:\n",
    "            for i, paragraph in enumerate(paragraph_ls):\n",
    "                masking_info = create_pretrain_instances(paragraph_ls, i, paragraph, n_seq, mask_prob, vocab_list)\n",
    "                for elem in masking_info:\n",
    "                    out_f.write(json.dumps(elem))\n",
    "                    out_f.write('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "PATH = '/home/henry/Documents/wrapper/source/\n",
    "#in_PATH = PATH + 'kowiki.txt'\n",
    "in_PATH = PATH + 'kowiki_sample.txt' # sample text\n",
    "sentences = []\n",
    "with open(in_PATH, 'r') as in_f:\n",
    "    for i, sent in enumerate(in_f):\n",
    "        # '': paragraph delimiter\n",
    "        if i == 7:\n",
    "            sentences.append('')\n",
    "        if i == 13:\n",
    "            break\n",
    "        sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['지미 카터\\n',\n",
       "  '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\\n',\n",
       "  '지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n',\n",
       "  '1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_ls = []\n",
    "paragraph = []\n",
    "for i, sent in enumerate(sentences):\n",
    "    sent = sent.strip()\n",
    "    ## blank means end of paragraph\n",
    "    if sent == '':\n",
    "        # if not the beggining of sentence\n",
    "        # it is the end of the paragraph\n",
    "        # generate new doc list\n",
    "        if 0 < len(paragraph):\n",
    "            paragraph_ls.append(paragraph)\n",
    "            paragraph = []\n",
    "            # check if 100thaousand paragraph\n",
    "            if 1e+5 < len(paragraph_ls):\n",
    "                break\n",
    "                \n",
    "    ## subwords in list is part of semantic token\n",
    "    # eg. ['▁지','미','▁카','터']\n",
    "    else:\n",
    "        pieces = vocab.encode_as_pieces(sent)\n",
    "        if 0 < len(pieces):\n",
    "            paragraph.append(pieces)\n",
    "if paragraph:\n",
    "    paragraph_ls.append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of paragraph: 2\n"
     ]
    }
   ],
   "source": [
    "print('num of paragraph:', len(paragraph_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 1st sentence\n",
      "['▁지', '미', '▁카', '터']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 1st sentence')\n",
    "print(paragraph_ls[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 2nd sentence\n",
      "['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 2nd sentence')\n",
    "print(paragraph_ls[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd paragraph - 1st sentence\n",
      "['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('2nd paragraph - 1st sentence')\n",
    "print(paragraph_ls[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token 253\n",
      "total_len 87 = 50 + 37\n",
      "[6, 15, 19, 26, 27, 30, 35, 43, 44, 47, 68, 69, 77] ['과', '▁일으', '▁1979', '국', '▁간의', '약', '▁또한', '기', '▁제한', '▁조', '▁인', '권을', '▁취임']\n"
     ]
    }
   ],
   "source": [
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki_sample.txt' \n",
    "out_PATH = '/home/henry/Documents/wrapper/source/out_kowiki_sample' + '_{}.json'\n",
    "\n",
    "count = 1\n",
    "n_seq = 200\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_PATH, out_PATH, count, n_seq, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "pretrain_output = []\n",
    "with open('/home/henry/Documents/wrapper/source/out_kowiki_sample_0.json', 'r') as f:\n",
    "    for line in f:\n",
    "        temp = json.loads(line)\n",
    "        pretrain_output.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, dict_keys(['tokens', 'segment', 'is_next', 'mask_idx', 'mask_label']))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrain_output), pretrain_output[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "['[CLS]', '▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁사랑', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '덫', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', 'ム', '▁완전히', '▁협', '상에', '攻', '인', '했다', '.', '[SEP]', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁반면', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]']\n",
      "\n",
      "segment\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "is_next\n",
      "True\n",
      "\n",
      "mask_idx\n",
      "[6, 15, 19, 26, 27, 30, 35, 43, 44, 47, 68, 69, 77]\n",
      "\n",
      "mask_label\n",
      "['과', '▁일으', '▁1979', '국', '▁간의', '약', '▁또한', '기', '▁제한', '▁조', '▁인', '권을', '▁취임']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in pretrain_output[0].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pretrain DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    eg. instance\n",
    "    {tokens:\n",
    "        ['[CLS]', '▁지', ', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', ...],\n",
    "    segment:\n",
    "        [0, 0, 0, 0, 0, 0, ..., 1, 1, 1],\n",
    "    is_next: True,\n",
    "    mask_idx: \n",
    "        [16, 21, ..., 41],\n",
    "    mask_label:\n",
    "        ['▁192', '▁1', '일', '▁~', '는', ..., '▁조지', '법을']}\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels_cls = []\n",
    "        self.label_lm_ls = []\n",
    "        self.sentence_ls = []\n",
    "        self.segments = []\n",
    "        \n",
    "        with open(infile, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                instance = json.loads(line)\n",
    "                self.labels_cls.append(instance['is_next'])\n",
    "                sentence = [vocab.piece_to_id(p) for p in instance['tokens']]\n",
    "                \n",
    "                self.sentence_ls.append(sentence)\n",
    "                self.segments.append(instance['segment'])\n",
    "                \n",
    "                mask_idx = np.array(instance['mask_idx'], dtype=np.int)\n",
    "                mask_label = np.array([vocab.piece_to_id(p) for p in instance['mask_label']], dtype=np.int)\n",
    "                label_lm = np.full(len(sentence), dtype=np.int, fill_value=-1)\n",
    "                label_lm[mask_idx] = mask_label\n",
    "                self.label_lm_ls.append(label_lm)\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels_cls) == len(self.label_lm_ls)\n",
    "        assert len(self.labels_cls) == len(self.sentence_ls)\n",
    "        assert len(self.labels_cls) == len(self.segments)\n",
    "        return len(self.labels_cls)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.labels_cls[idx]),\n",
    "                torch.tensor(self.label_lm_ls[idx]),\n",
    "                torch.tensor(self.sentence_ls[idx]),\n",
    "                torch.tensor(self.segments[idx]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab\n",
    "labels_cls = []; labels_lm = []; sentences = []; segments = [];\n",
    "\n",
    "with open(infile, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        instance = json.loads(line)\n",
    "        labels_cls.append(instance['is_next'])\n",
    "        \n",
    "        sentence = [vocab.piece_to_id(p) for p in instance['tokens']]\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        segments.append(instance['segment'])\n",
    "        mask_idx = np.array(instance['mask_idx'], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in instance['mask_label']], dtype=np.int)\n",
    "        \n",
    "        label_lm = np.full(len(sentence), dtype=np.int, fill_value=-1)\n",
    "        label_lm[mask_idx] = mask_label\n",
    "        labels_lm.append(label_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is mask label: [3635, 1226, 2962, 3634, 2712, 3818, 274, 3605, 1983, 53, 44, 904, 2612] \n",
      "this is idx of mask label in seq: [6, 15, 19, 26, 27, 30, 35, 43, 44, 47, 68, 69, 77] \n",
      "this is sentence with voc_idx: [5, 322, 1470, 41, 3676, 3718, 3635, 668, 2652, 3619, 167, 1321, 142, 3710, 3598, 1226, 4162, 3589, 3590, 1421, 3616, 456, 3918, 3698, 10, 224, 3634, 2712, 2791, 3667, 3818, 9, 1436, 2518, 3590, 6864, 1271, 3635, 30, 3610, 3741, 2918, 108, 6771, 2056, 623, 1790, 7079, 3619, 31, 3590, 4, 210, 3705, 3593, 1908, 592, 1808, 312, 408, 50, 44, 3821, 82, 3704, 134, 967, 247, 44, 904, 18, 3784, 3605, 233, 3366, 528, 3595, 2897, 165, 776, 869, 74, 4078, 3633, 1232, 115, 3682, 1844, 3590, 4]\n"
     ]
    }
   ],
   "source": [
    "print('this is mask label: {}'.format([vocab.piece_to_id(p) for p in instance['mask_label']]),\\\n",
    "      '\\nthis is idx of mask label in seq: {}'.format(instance['mask_idx']),\\\n",
    "     '\\nthis is sentence with voc_idx: {}'.format([vocab.piece_to_id(p) for p in instance[\"tokens\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True),\n",
       " tensor([  -1,   -1,   -1,   -1,   -1,   -1, 3635,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1, 1226,   -1,   -1,   -1, 2962,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1, 3634, 2712,   -1,   -1, 3818,   -1,   -1,   -1,   -1,  274,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1, 3605, 1983,   -1,   -1,   53,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   44,  904,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1, 2612,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1]),\n",
       " tensor([   5,  322, 1470,   41, 3676, 3718, 3635,  668, 2652, 3619,  167, 1321,\n",
       "          142, 3710, 3598, 1226, 4162, 3589, 3590, 1421, 3616,  456, 3918, 3698,\n",
       "           10,  224, 3634, 2712, 2791, 3667, 3818,    9, 1436, 2518, 3590, 6864,\n",
       "         1271, 3635,   30, 3610, 3741, 2918,  108, 6771, 2056,  623, 1790, 7079,\n",
       "         3619,   31, 3590,    4,  210, 3705, 3593, 1908,  592, 1808,  312,  408,\n",
       "           50,   44, 3821,   82, 3704,  134,  967,  247,   44,  904,   18, 3784,\n",
       "         3605,  233, 3366,  528, 3595, 2897,  165,  776,  869,   74, 4078, 3633,\n",
       "         1232,  115, 3682, 1844, 3590,    4]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = '/home/henry/Documents/wrapper/source/out_kowiki_sample_0.json'\n",
    "dataset = PretrainDataset(vocab, infile)\n",
    "sample_dataset = next(iter(dataset))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_collate_fn(inputs):\n",
    "    \"\"\"\n",
    "    padding batch\n",
    "    \"\"\"\n",
    "    labels_cls, labels_lm, inputs, segments = list(zip(*inputs))\n",
    "    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1)\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
    "    \n",
    "    batch = [\n",
    "        torch.stack(labels_cls, dim=0),\n",
    "        labels_lm,\n",
    "        inputs,\n",
    "        segments,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = PretrainDataset(vocab, PATH+'kowiki_bert_0.json')\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size\\\n",
    "                                           , shuffle=True, collate_fn=pretrain_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader):\n",
    "    loss_ls = []\n",
    "    model.train()\n",
    "    print('model train')\n",
    "    for i, value in enumerate(train_loader):\n",
    "        labels_cls, labels_lm, inputs, segments = map(lambda x: x.to(config.device), value)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, segments)\n",
    "        logits_cls, logits_lm = outputs[0], outputs[1]\n",
    "        \n",
    "        loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "        loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1))\n",
    "        loss = loss_cls + loss_lm\n",
    "        \n",
    "        loss_val = loss_lm.item()\n",
    "        loss_ls.append(loss_val)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.mean(loss_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(d_ff=1024, d_head=64, d_hidn=256, device=device(type='cpu'), dropout=0.1, i_pad=0, layer_norm_epsilon=1e-12, n_enc_seq=256, n_enc_vocab=8007, n_head=4, n_layer=6, n_seg_type=2)\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model train\n"
     ]
    }
   ],
   "source": [
    "model = BERTpretrain(config)\n",
    "save_pretrain = PATH + 'bert_pretrain_weights.pkl'\n",
    "best_epoch, best_loss = 0, 0\n",
    "if os.path.isfile(save_pretrain):\n",
    "    best_epoch, best_loss = model.bert.load(save_pretrain)\n",
    "    best_epoch += 1\n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_ls = []\n",
    "offset = best_epoch\n",
    "for step in range(n_epoch):\n",
    "    epoch = step + offset\n",
    "#     if 0 < step:\n",
    "#         del train_loader\n",
    "#         dataset = PretrainDataset(vocab, PATH + 'kowiki_bert_0.json')\n",
    "#         train_loader = DataLoader(dataset, batch_size=batch_size, \\\n",
    "#                                   suffle=True, collate_fn=pretrain_collate_fn)\n",
    "    loss = train_epoch(config, epoch, model, criterion_lm, criterion_cls,\\\n",
    "                      optimizer, train_loader)\n",
    "    loss_ls.append(loss)\n",
    "    model.bert.save(epoch, loss, save_pretrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
