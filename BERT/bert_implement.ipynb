{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reference\n",
    "    - https://github.com/HMJiangGatech/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/pregenerate_training_data.py\n",
    "    - https://paul-hyun.github.io/bert-01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "import sentencepiece as spm\n",
    "\n",
    "VOCAB_PATH = \"/home/henry/Documents/wrapper/source\"\n",
    "vocab_file = f\"{VOCAB_PATH}/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "config = dict({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_seg_type\": 2,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(d_ff=1024, d_head=64, d_hidn=256, dropout=0.1, i_pad=0, layer_norm_epsilon=1e-12, n_enc_seq=256, n_enc_vocab=8007, n_head=4, n_layer=6, n_seg_type=2)\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    outputs: [bs, len_seq, d_hidn] <- 잘 임베딩된 input seq\n",
    "    ointput_cls = outputs[:, 0].contiguous(): [bs, d_hidn]\n",
    "    - classification은 [cls] token의 임베딩만 사용\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(slef, inputs, segment):\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
    "        ointput_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls)\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'state_dict': self.state_dict()\n",
    "                   }, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save['state_dict'])\n",
    "        return save['epoch'], save['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTpretrain(nn.Module):\n",
    "    \"\"\"\n",
    "    self.feedforward_lm.weight\n",
    "    - transformer encoder의 pretrained embedding layer weight 사용\n",
    "    logits_cls: [bs, 2]\n",
    "    - binary classification\n",
    "    logits_lm: [bs, len_enc_seq, n_enc_vocab]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert  = BERT(self.config)\n",
    "        # cls\n",
    "        self.feedforward_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n",
    "        # lm\n",
    "        self.feedforward_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n",
    "        self.feedforward_lm.weight = self.bert.encoder.enc_emb.weight\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        logits_cls = self.feedforward_cls(outputs)\n",
    "        logits_lm = self.feedforward_lm(outputs)\n",
    "        return logits_cls, logits_lm, attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "- https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/trainer/pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    masking subwords(15% of entire subwords)\n",
    "    - mask_cnt: len(subwords) * 0.15\n",
    "    - [MASK]: 80% of masking candidate token\n",
    "    - original token: 10% of masking candidate token\n",
    "    - another token: 10% of masking candidate token\n",
    "    \"\"\"\n",
    "    candidate_idx = []\n",
    "\n",
    "    ## subwords in the same list augment a sementic word \n",
    "    ## eg. [[0], [1], [2], [4, 5]] -> token_idx 4 + 5 is semantic word\n",
    "    # A list represent a sementic word\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            continue\n",
    "        if 0 < len(candidate_idx) and token.find(u'\\u2581') < 0: #  LOWER ONE EIGHTH BLOCK\n",
    "#        if 0 < len(candidate_idx) and token.find('_') < 0: #  test code\n",
    "            candidate_idx[-1].append(i)\n",
    "        else:\n",
    "            candidate_idx.append([i])\n",
    "    np.random.shuffle(candidate_idx)\n",
    "\n",
    "    mask_lms = []\n",
    "    for idx_set in candidate_idx:\n",
    "        # check if len(mask_lms) exceeds threshold\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(idx_set) > mask_cnt:\n",
    "            continue\n",
    "\n",
    "        ## masking subwords with 15% probability\n",
    "        ## mask_cnt is len(subwords) * 0.15 \n",
    "        # iter subwords idx\n",
    "        for sub_idx in idx_set:\n",
    "            masked_token = None\n",
    "\n",
    "            ### assign value to masked token: [MASK], original token, random token\n",
    "            # 80% of masking candidate are replaced with '[MASK]' token\n",
    "            if np.random.uniform() < 0.8:\n",
    "                masked_token = '[MASK]'\n",
    "            # remainng 20% of masking candidate\n",
    "            else:\n",
    "                # 10% of remaining preserve original token\n",
    "                if np.random.uniform() < 0.5:\n",
    "                    masked_token = tokens[sub_idx]\n",
    "                # 10% of ones are replaced with rnadom token    \n",
    "                else:\n",
    "                    masked_token = np.random.choice(vocab_list)\n",
    "\n",
    "                ### replace subword with masked_token value    \n",
    "                mask_lms.append({'idx': sub_idx, 'label':tokens[sub_idx]})\n",
    "                tokens[sub_idx] = masked_token\n",
    "                \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x['idx'])\n",
    "    mask_idx = [mask_dict['idx'] for mask_dict in mask_lms]\n",
    "    mask_label = [mask_dict['label'] for mask_dict in mask_lms]\n",
    "#     print(candidate_idx)\n",
    "#     print(mask_lms)\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_token(tokenA, tokenB, max_seq):\n",
    "    \"\"\"\n",
    "    truncate long sequence\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_len = len(tokenA) + len(tokenB)\n",
    "        print('max token {}\\ntotal_len {} = {} + {}'.format(max_seq, total_len, len(tokenA), len(tokenB)))\n",
    "        if total_len <= max_seq:\n",
    "            break\n",
    "        if len(tokenA) > len(tokenB):\n",
    "            tokenA.pop()\n",
    "        else:\n",
    "            tokenB.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['_I am', '_on', '_it', '_so wh', 'at',]\n",
    "tokens = ['_I am', '_on', '_it', '[CLS]', '_so wh', 'at',]\n",
    "mask_cnt = int(len(tokens) * 0.15)+1\n",
    "create_pretrain_mask(tokens, mask_cnt, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# pretrain dataset for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['▁지', '미', '▁카', '터'], ['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.'], ['▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카', '운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.'], ['▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.'], ['▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '▁당선', '되었다', '.'], ['▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '▁반', '대로', '▁무', '산', '되었다', '.'], ['▁카', '터', '는', '▁이집', '트', '와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠', '프', '▁데이', '비', '드', '에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.'], ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']]]\n"
     ]
    }
   ],
   "source": [
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki_sample.txt' \n",
    "out_PATH = '/home/henry/Documents/wrapper/source/out_kowiki_sample' + '_{}.json'\n",
    "\n",
    "count = 1\n",
    "n_seq = 10\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_PATH, out_PATH, count, n_seq, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(paragraph_ls, paragraph_idx, paragraph, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    create NSP train set\n",
    "    \"\"\"\n",
    "    # 3 special token: [CLS], [SEP] for sent A, [SEP] for sent B\n",
    "    max_seq_len = n_seq - 2 - 1\n",
    "    target_seq_len = max_seq_len # [CLS], segmentA, segmentA, ..., [SEP], segmentB, segmentB, ...\n",
    "\n",
    "    instances = []\n",
    "    temp_sentence = []\n",
    "    temp_sent_seq_length = 0 # num of tokens\n",
    "\n",
    "    max_num_tokens = 256\n",
    "    target_seq_len = np.random.randint(2, max_num_tokens) # min len of tokens\n",
    "    for i, sent in enumerate(paragraph):\n",
    "        ## A. not the last sentence of the paragraph\n",
    "        temp_sentence.append(sent)\n",
    "        temp_sent_seq_length += len(sent)\n",
    "\n",
    "        ## B. check if it is the last sentence of the paragraph\n",
    "        ## or temp_sent_seq_length is longer than or equal to target_seq_len\n",
    "        if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "            if temp_sentence:\n",
    "                ## A. sentence A segment: from 0 to a_end\n",
    "                a_end = 1\n",
    "                if len(temp_sentence) != 1:\n",
    "                    a_end = np.random.randint(1, len(temp_sentence))\n",
    "                # append the sentences to tokenA \n",
    "                # from the front to the back\n",
    "                tokenA = []\n",
    "                for _, s in enumerate(temp_sentence[:a_end]):\n",
    "                    tokenA.extend(s)\n",
    "\n",
    "                ## B. sentence B segment\n",
    "                tokenB = []\n",
    "                # A. Actual next\n",
    "                if len(temp_sentence) > 1 and np.random.uniform() >= 0.5:\n",
    "                    is_next = True\n",
    "                    for j in range(a_end, len(temp_sentence)):\n",
    "                        tokenB.extend(temp_sentence[j])\n",
    "                # B. random next\n",
    "                else:\n",
    "                    is_next = False\n",
    "                    tokenB_len = target_seq_len - len(tokenA)\n",
    "                    random_para_idx = para_idx\n",
    "                    while para_idx == random_para_idx:\n",
    "                        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "                    random_para = paragraph[random_para_idx]\n",
    "\n",
    "                    random_start = np.random.randint(0, len(random_para))\n",
    "                    for j in range(random_start, len(random_para)):\n",
    "                        tokenB.extend(random_para[j])\n",
    "\n",
    "                truncate_token(tokenA, tokenB, max_seq)\n",
    "                assert 0 < len(tokenA)\n",
    "                assert 0 < len(tokenB)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokenA + [\"[SEP]\"] + tokenB + [\"[SEP]\"]\n",
    "                segment = [0]*(len(tokenA)  + 2) + [1]*(len(tokenB) + 1)\n",
    "                \n",
    "                tokens, mask_idx, mask_label = \\\n",
    "                    create_pretrain_mask(tokens, int((len(tokens)-3)*mask_prob), vocab_list)\n",
    "                instance = {\n",
    "                    'tokens': tokens,\n",
    "                    'segment': segment,\n",
    "                    'is_next': is_next,\n",
    "                    'mask_idx': mask_idx,\n",
    "                    'mask_label': mask_label\n",
    "                }\n",
    "\n",
    "                instances.append(instance)\n",
    "\n",
    "            # reset segment candidate\n",
    "            temp_sentence = []\n",
    "            temp_sent_seq_length = 0\n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx]\n",
    "print(len(paragraph), paragraph[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 0\n",
    "    - add 1st sequence to meet target sequence length\n",
    "    - max_len_doc == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target seq len: 253\n"
     ]
    }
   ],
   "source": [
    "n_seq = 256\n",
    "max_seq = n_seq - 3\n",
    "target_seq_len = max_seq\n",
    "\n",
    "instances = []\n",
    "temp_sentences = []\n",
    "temp_sent_seq_length = 0\n",
    "print('target seq len:', target_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 50\n",
      "# of sentences in the paragraph: 6\n",
      "temp_sentences len: 1 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']]\n"
     ]
    }
   ],
   "source": [
    "# the case of 1st sentence in 2nd paragraph \n",
    "para_idx = 1\n",
    "paragraph = paragraph_ls[para_idx] # 2nd paragraph\n",
    "\n",
    "temp_sentences = []\n",
    "i = 0 \n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 1\n",
    "    - add one more sentece to segments A to meet target sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 87\n",
      "# of sentences in the paragraph: 6\n",
      "temp_sentences len: 2 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 161\n",
      "# of sentences in the paragraph: 6\n",
      "temp_sentences len: 3 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.'], ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current seq length: 311\n",
      "# of sentences in the paragraph: 6\n",
      "temp_sentences len: 4 \n",
      " [['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.'], ['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.'], ['▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.'], ['▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤', '던', '▁대통령', '▁중', '▁하나', '다', '.', '▁인', '권', '▁문제', '와', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '▁한', '때', '▁한', '미', '▁관계', '가', '▁불', '편', '하기도', '▁했다', '.', '▁1978', '년', '▁대한민국', '에', '▁대한', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주', '한', '미', '군을', '▁철', '수', '하기로', '▁했다', '.', '▁그러나', '▁주', '한', '미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '▁반', '대에', '▁부', '딪', '혀', '▁주', '한', '미', '군은', '▁완전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '▁정', '권의', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '▁회복', '되었다', '.']]\n",
      "Is it the last sentence of the paragraph \n",
      "or current chunk longer than target_seq_len? True\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "temp_sentences.append(paragraph[i]) # 1st sentence\n",
    "temp_sent_seq_length += len(paragraph[i])\n",
    "\n",
    "print('current seq length:', temp_sent_seq_length)\n",
    "print('# of sentences in the paragraph: {}'.format(len(paragraph)))\n",
    "print('temp_sentences len: {}'.format(len(temp_sentences)), '\\n', temp_sentences)\n",
    "\n",
    "if i == len(paragraph) - 1 or temp_sent_seq_length >= target_seq_len:\n",
    "    print('Is it the last sentence of the paragraph \\nor current chunk longer than target_seq_len?', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenA consists of 3 sentences \n",
      " ['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "if 0 < len(temp_sentences):\n",
    "    a_end = 1\n",
    "    if 1 < len(temp_sentences):\n",
    "        a_end = np.random.randint(1, len(temp_sentences))\n",
    "        \n",
    "    tokenA = []\n",
    "    for j in range(a_end):\n",
    "        tokenA.extend(temp_sentences[j]) # convert sentence to segment\n",
    "        \n",
    "print('tokenA consists of {} sentences'.format(a_end), '\\n', tokenA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB random sentence with 1/2 probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenB avaliable length: 92\n",
      "tokenB len 97 should be longer than 92, \n",
      "or truncate segments\n",
      "['▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출']\n"
     ]
    }
   ],
   "source": [
    "tokenB = []\n",
    "if len(temp_sentences) == 1 or np.random.uniform() < 0.5:\n",
    "    is_next = False\n",
    "    tokenB_len = target_seq_len - len(tokenA) # minumum len of tokenB\n",
    "    print('tokenB avaliable length: {}'.format(tokenB_len))\n",
    "    \n",
    "    # choose sentence from other paragraph\n",
    "    random_para_idx = para_idx\n",
    "    while para_idx == random_para_idx:\n",
    "        random_para_idx = np.random.randint(0, len(paragraph_ls))\n",
    "    # this is random paragraph    \n",
    "    random_paragraph = paragraph_ls[random_para_idx]\n",
    "    \n",
    "    # add a series of sentences from random paragragraph\n",
    "    random_start = np.random.randint(0, len(random_paragraph))\n",
    "    for j in range(random_start, len(random_paragraph)):\n",
    "        tokenB.extend(random_paragraph[j])\n",
    "        if len(tokenB) > tokenB_len:\n",
    "            break\n",
    "print('tokenB len {} should be longer than {}, \\nor truncate segments'.format(len(tokenB), tokenB_len))\n",
    "print(tokenB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenB the sentences right after tokenA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the end of the tokenA: \n",
      "['국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.']\n",
      "the beggining of the tokenB: \n",
      "['▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출']\n"
     ]
    }
   ],
   "source": [
    "is_next = 1\n",
    "for j in range(a_end, len(temp_sentences)):\n",
    "    tokenB.extend(temp_sentences[j])\n",
    "print('the end of the tokenA: \\n{}'.format(tokenA[-10:]))\n",
    "print('the beggining of the tokenB: \\n{}'.format(tokenB[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- truncate long segmentsA, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token 253\n",
      "total_len 253 = 127 + 126\n"
     ]
    }
   ],
   "source": [
    "truncate_token(tokenA, tokenB, max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 < len(tokenA)\n",
    "assert 0 < len(tokenB)\n",
    "tokens = ['[CLS]'] + tokenA + ['[SEP]'] + tokenB + ['[SEP]']\n",
    "segment = [0]*(len(tokenA)  + 2) + [1]*(len(tokenB) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# make pre train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
    "    \"\"\"\n",
    "    read text and return train data set format\n",
    "    \"\"\"\n",
    "    vocab_list = []\n",
    "    for id_ in range(vocab.get_piece_size()):\n",
    "        if not vocab.is_unknown(id_):\n",
    "            vocab_list.append(vocab.id_to_piece(id_))\n",
    "    \n",
    "    paragraph_ls = []\n",
    "    with open(in_file, 'r') as in_f:\n",
    "        paragraph = []\n",
    "        for i, sent in enumerate(in_f):\n",
    "            sent = sent.strip()\n",
    "            \n",
    "            ## blank means end of the paragraph\n",
    "            if sent == '':\n",
    "                # if not the beggining of the paragraph\n",
    "                # it is the end of the paragraph\n",
    "                if 0 < len(paragraph):\n",
    "                    paragraph_ls.append(paragraph)\n",
    "                    paragraph = [] # generate new paragraph list\n",
    "                    # check if exceeding 100 thaousand paragraphs\n",
    "                    if 1e+5 < len(paragraph_ls): \n",
    "                        break \n",
    "                        \n",
    "            ## subwords in list is part of semantic token\n",
    "            # eg. ['▁지','미','▁카','터']\n",
    "            else:\n",
    "                pieces = vocab.encode_as_pieces(sent)\n",
    "                if 0 < len(pieces):\n",
    "                    paragraph.append(pieces)\n",
    "        if paragraph:\n",
    "            paragraph_ls.append(paragraph)\n",
    "    # masking def: create_pretrain_mask\n",
    "    for index in range(count):\n",
    "        output = out_file.format(index)\n",
    "        if os.path.isfile(output):\n",
    "            continue\n",
    "        with open(output, 'w') as out_f:\n",
    "            for i, paragraph in enumerate(paragraph_ls):\n",
    "                print(paragraph)\n",
    "                masking_info = create_pretrain_instances(paragraph_ls, i, paragraph, n_seq, mask_prob, vocab_list)\n",
    "                for elem in masking_info:\n",
    "                    out_f.write(json.dumps(elem))\n",
    "                    out_f.write('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "#in_PATH = '/home/henry/Documents/wrapper/source/kowiki.txt'\n",
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki_sample.txt' # sample text\n",
    "sentences = []\n",
    "with open(in_PATH, 'r') as in_f:\n",
    "    for i, sent in enumerate(in_f):\n",
    "        # '': paragraph delimiter\n",
    "        if i == 7:\n",
    "            sentences.append('')\n",
    "        if i == 13:\n",
    "            break\n",
    "        sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['지미 카터\\n',\n",
       "  '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\\n',\n",
       "  '지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\\n',\n",
       "  '1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\\n'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_ls = []\n",
    "paragraph = []\n",
    "for i, sent in enumerate(sentences):\n",
    "    sent = sent.strip()\n",
    "    ## blank means end of paragraph\n",
    "    if sent == '':\n",
    "        # if not the beggining of sentence\n",
    "        # it is the end of the paragraph\n",
    "        # generate new doc list\n",
    "        if 0 < len(paragraph):\n",
    "            paragraph_ls.append(paragraph)\n",
    "            paragraph = []\n",
    "            # check if 100thaousand paragraph\n",
    "            if 1e+5 < len(paragraph_ls):\n",
    "                break\n",
    "                \n",
    "    ## subwords in list is part of semantic token\n",
    "    # eg. ['▁지','미','▁카','터']\n",
    "    else:\n",
    "        pieces = vocab.encode_as_pieces(sent)\n",
    "        if 0 < len(pieces):\n",
    "            paragraph.append(pieces)\n",
    "if paragraph:\n",
    "    paragraph_ls.append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of paragraph: 2\n"
     ]
    }
   ],
   "source": [
    "print('num of paragraph:', len(paragraph_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 1st sentence\n",
      "['▁지', '미', '▁카', '터']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 1st sentence')\n",
    "print(paragraph_ls[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st paragraph - 2nd sentence\n",
      "['▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('1st paragraph - 2nd sentence')\n",
    "print(paragraph_ls[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd paragraph - 1st sentence\n",
      "['▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "print('2nd paragraph - 1st sentence')\n",
    "print(paragraph_ls[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_PATH = '/home/henry/Documents/wrapper/source/kowiki_sample.txt' \n",
    "out_PATH = '/home/henry/Documents/wrapper/source/out_kowiki_sample' + '_{}.json'\n",
    "\n",
    "count = 1\n",
    "n_seq = 10\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_PATH, out_PATH, count, n_seq, mask_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "pretrain_output = []\n",
    "with open('/home/henry/Documents/wrapper/source/out_kowiki_sample_0.json', 'r') as f:\n",
    "    for line in f:\n",
    "        temp = json.loads(line)\n",
    "        pretrain_output.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, dict_keys(['tokens', 'segment', 'is_next', 'mask_idx', 'mask_label']))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrain_output), pretrain_output[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "['[CLS]', '▁지', '미', '▁카', '터', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', 'Θ', '▁M', '▁2007', '▁)', 'び', '▁민주', '당', '▁출신', '▁미국', 'г', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '[SEP]', '▁지', '▁초등학교이다', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카', '운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '慚', '자', '력', '眠', '잠', '수', '함', '의', '▁승', '무', '원으로', '녘', '하였다', '.', '▁195', '3', '년', '懸', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', '岳', '따', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.', '▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '環', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', 'A', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '팟', '▁주', '▁지', '사', '▁선거', '에', '創', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '際', '▁역임', '했다', '▁신', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '식', '년부터', '▁1975', '년까지', '▁운동', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]']\n",
      "\n",
      "segment\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "is_next\n",
      "True\n",
      "\n",
      "mask_idx\n",
      "[16, 21, 22, 23, 25, 30, 46, 47, 48, 54, 72, 83, 86, 87, 89, 94, 100, 106, 112, 117, 134, 135, 153, 170, 178, 184, 185, 193, 196, 199, 206, 210, 214, 218, 241]\n",
      "\n",
      "mask_label\n",
      "['▁192', '▁1', '일', '▁~', '는', '▁3', '미', '▁카', '터', '터', '▁졸업', '원', '·', '잠', '함', '▁일', '▁미국', '하였고', '화', '▁돈', 'an', 'ut', '▁의원', '▁되어', '아', '▁낙', '선', '사를', '.', '▁되', '원의', '▁연', '▁1971', '▁조지', '법을']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in pretrain_output[0].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pretrain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels_cls = []\n",
    "        self.labels_lm = []\n",
    "        self.sentences = []\n",
    "        self.segments = []\n",
    "        \n",
    "        with open(infile, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                instance = json.loads(line)\n",
    "                self.labels_cls.append(instance['is_next'])\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
