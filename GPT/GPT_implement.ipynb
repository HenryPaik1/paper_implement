{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_sinusoid_encoding_table(n_seq, emb_d):\n",
    "    \"\"\"\n",
    "    n_seq: seq_len\n",
    "    emb_d: dim of sinusoide table\n",
    "        - equal to the dim of word embedded weight mat \n",
    "    \"\"\"\n",
    "    def _cal_angle(position, ith_emb_fature):\n",
    "        return position / np.power(10000, 2 * (ith_emb_fature // 2) / emb_d)\n",
    "    \n",
    "    def _get_position_angle_vec(position):\n",
    "        return [_cal_angle(position, ith_emb_feature) for ith_emb_feature in range(emb_d)]\n",
    "    \n",
    "    sinusoid_table = np.array([_get_position_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad): # i_pad=0\n",
    "    \"\"\"\n",
    "    seq_q: query sequence(not embbed)\n",
    "    seq_k: key sequence\n",
    "    i_pad: padding vocab_idx \n",
    "        - eg. 0\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    \"\"\"\n",
    "    mask upper triangular part\n",
    "    seq: decoder sequence\n",
    "        - not word embedding\n",
    "    \"\"\"\n",
    "    triangular_mask = torch.ones_like(seq).unsqueeze(-1).expand((seq.size(0), seq.size(1), seq.size(1)))\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: use types.SimpleNamespace\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.k_dim**0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        attn_mask: encoder part and decoder part has different one\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e+9)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        return context, attn_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "# for dot notation for dict\n",
    "conf = {\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "}\n",
    "config = SimpleNamespace(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer decoder part for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, dec_inputs, self_attn_mask):\n",
    "        \"\"\"\n",
    "        shape\n",
    "        - self_att_outputs: bs, n_dec_seq, d_hidn\n",
    "        - self_attn_prob: bs, n_head, dec_seq_len, dec_seq_len \n",
    "        - ffn_outputs: bs, dec_seq_len, d_hidn\n",
    "        \"\"\"\n",
    "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
    "        ffn_outputs = self.pos_ffn(self_att_outputs)\n",
    "        ffn_outputs = self.layer_norm3(self_att_outputs + ffn_outputs)\n",
    "        return ffn_outputs, self_attn_prob\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    consisting of decoder layers\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
    "        # get through n multihead-decoder layers\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, dec_inputs):\n",
    "        \"\"\"\n",
    "        dec_outputs: bs, dec_seq_len, d_hidn\n",
    "         - torch.matmul(attn_prob, V)\n",
    "         - shape of which is equal to one of inputs\n",
    "        self_attn_prob: bs, dec_seq_len, dec_seq_len\n",
    "         - softmax(score)\n",
    "        dec_attn_pad_mask: bs, dec_seq_len, dec_seq_len \n",
    "        dec_attn_decoder_mask: bs, dec_seq_len, dec_seq_len\n",
    "        \"\"\"\n",
    "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype)\\\n",
    "        .expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "        \n",
    "        # decoder first input\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
    "        \n",
    "        # masking both padded part of key vector and decoder input part\n",
    "        # position wise sum of which is zero\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        \n",
    "        # get thrgough a series of multihead-decoder layers\n",
    "        self_attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "        return dec_outputs, self_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.decoder = Decoder(self.config)\n",
    "        \n",
    "    def forward(self, dec_inputs):\n",
    "        dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)\n",
    "        return dec_outputs, dec_self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'state_dict': self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save['state_dict'])\n",
    "        return save['epoch'], save['loss']\n",
    "    \n",
    "class GPTpretrain(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.gpt = GPT(self.config)\n",
    "        \n",
    "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias=False)\n",
    "        self.projection_lm.weight = self.gpt.decoder.dec_emb.weight\n",
    "    \n",
    "    def forward(self, dec_inputs):\n",
    "        dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)\n",
    "        logits_lm = self.projection_lm(dec_outputs)\n",
    "        return logits_lm[:, :-1, :].contiguous(), dec_self_attn_probs  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
