{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_sinusoid_encoding_table(n_seq, emb_d):\n",
    "    \"\"\"\n",
    "    n_seq: seq_len\n",
    "    emb_d: dim of sinusoide table\n",
    "        - equal to the dim of word embedded weight mat \n",
    "    \"\"\"\n",
    "    def _cal_angle(position, ith_emb_fature):\n",
    "        return position / np.power(10000, 2 * (ith_emb_fature // 2) / emb_d)\n",
    "    \n",
    "    def _get_position_angle_vec(position):\n",
    "        return [_cal_angle(position, ith_emb_feature) for ith_emb_feature in range(emb_d)]\n",
    "    \n",
    "    sinusoid_table = np.array([_get_position_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad): # i_pad=0\n",
    "    \"\"\"\n",
    "    seq_q: query sequence(not embbed)\n",
    "    seq_k: key sequence\n",
    "    i_pad: padding vocab_idx \n",
    "        - eg. 0\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_decoder_mask(seq):\n",
    "    \"\"\"\n",
    "    mask upper triangular part\n",
    "    seq: decoder sequence\n",
    "        - not word embedding\n",
    "    \"\"\"\n",
    "    triangular_mask = torch.ones_like(seq).unsqueeze(-1).expand((seq.size(0), seq.size(1), seq.size(1)))\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: use types.SimpleNamespace\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.k_dim**0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        attn_mask: encoder part and decoder part has different one\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e+9)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        return context, attn_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.decoder = Decoder(self.config)\n",
    "        \n",
    "    def forward(self, dec_inputs):\n",
    "        dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)\n",
    "        return dec_outputs, dec_self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'state_dict': self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save['state_dict'])\n",
    "        return save['epoch'], save['loss']\n",
    "    \n",
    "class GPTpretrain(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.gpt = GPT(self.config)\n",
    "        \n",
    "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias=False)\n",
    "        self.projection_lm.weight = self.gpt.decoder.dec_emb.weight\n",
    "    \n",
    "    def forward(self, dec_inputs):\n",
    "        dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)\n",
    "        logits_lm = self.projection_lm(dec_outputs)\n",
    "        return logits_lm[:, :-1, :].contiguous(), dec_self_attn_probs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
